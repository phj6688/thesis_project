{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C  N  N\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "import random\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None        \n",
    "        self.label_mapping = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        log_dir = f\"logs/fit/run_only_once\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix = np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for j, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x_matrix[i, j, :] = self.w2v[word]\n",
    "                else:\n",
    "                    continue        \n",
    "                y_matrix[i,label] = 1.0    \n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "        \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "            hist_dict = {}\n",
    "            res_dict = {}\n",
    "            best_val_loss = float('inf')\n",
    "            for i in range(n):\n",
    "                print(f'Run {i+1} of {n}')\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "                res = self.evaluate(test_x, test_y)\n",
    "                res_dict[i+1] = res\n",
    "                if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                    best_val_loss = self.history.history['val_loss'][-1]\n",
    "                    self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "                self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "            \n",
    "            avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "            \n",
    "            # Save the average results to disk\n",
    "            os.makedirs(\"results\", exist_ok=True)\n",
    "            with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "                for key, value in avg_dict.items():\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            K.clear_session()\n",
    "            \n",
    "            return hist_dict, res_dict, avg_dict\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20,batch_size_insert=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_insert = batch_size_insert\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None        \n",
    "        self.label_mapping = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']        \n",
    "        self.callbacks =None\n",
    "        \n",
    "\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix =        np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i in range(0, num_lines, self.batch_size_insert):\n",
    "                df_batch = df.iloc[i:i+self.batch_size_insert]\n",
    "                batch_size = len(df_batch)\n",
    "                x_batch = np.zeros((batch_size, self.max_seq_len, 300))\n",
    "                y_batch = np.zeros((batch_size, self.n_classes))\n",
    "\n",
    "                for j, row in df_batch.iterrows():\n",
    "                    label = row[0]\n",
    "                    sentence = row[1]\n",
    "                    if isinstance(sentence, str):\n",
    "                        words = sentence.split()[:self.max_seq_len]\n",
    "                        for k, word in enumerate(words):\n",
    "                            if word in self.w2v:\n",
    "                                x_batch[j-i, k, :] = self.w2v[word]\n",
    "                    else:\n",
    "                        continue        \n",
    "                    y_batch[j-i,label] = 1.0\n",
    "\n",
    "                x_matrix[i:i+batch_size] = x_batch\n",
    "                y_matrix[i:i+batch_size] = y_batch\n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "    \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "        \n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_cnn()\n",
    "                continue\n",
    "            res = self.evaluate(test_x, test_y)\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "        \n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "        \n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'data/original/agnews/train.csv'\n",
    "test_path   = 'data/original/agnews/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "name = 'agnews'\n",
    "max_seq_len = 150\n",
    "batch_size = 8\n",
    "epochs = 30\n",
    "cnn = CNN(dims=300, w2v_path=w2v_path, max_seq_len=20, batch_size=128, epochs=20)\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, n_classes = cnn.insert_values(train_path,test_path)\n",
    "hist_dict, res_dict, avg_dict = cnn.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, name, n=3)\n",
    "\n",
    "\n",
    "# model = CNN(dims=300, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, w2v_path=w2v_path)\n",
    "# train_x, train_y, test_x, test_y, val_x, val_y, n_classes = model.insert_values(train_path,test_path)\n",
    "# his,res,avg = model.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, n=3)\n",
    "# print (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "import random\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20,chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None                \n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']        \n",
    "        self.callbacks =None\n",
    "        \n",
    "\n",
    "    def build_lstm(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        lstm_1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_layer)\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(lstm_1)\n",
    "        lstm_2 = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(dropout_out1)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(lstm_2)\n",
    "        dense_1 = layers.Dense(20, activation='relu')(dropout_out2)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dense_1)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        lstm_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        lstm_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #lstm_model.summary()\n",
    "        self.model = lstm_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix = np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i in range(0, num_lines, self.chunk_size):\n",
    "                df_batch = df.iloc[i:i+self.chunk_size]\n",
    "                batch_size = len(df_batch)\n",
    "                x_batch = np.zeros((batch_size, self.max_seq_len, 300))\n",
    "                y_batch = np.zeros((batch_size, self.n_classes))\n",
    "\n",
    "                for j, row in df_batch.iterrows():\n",
    "                    label = row[0]\n",
    "                    sentence = row[1]\n",
    "                    if isinstance(sentence, str):\n",
    "                        words = sentence.split()[:self.max_seq_len]\n",
    "                        for k, word in enumerate(words):\n",
    "                            if word in self.w2v:\n",
    "                                x_batch[j-i, k, :] = self.w2v[word]\n",
    "                    else:\n",
    "                        continue        \n",
    "                    y_batch[j-i,label] = 1.0\n",
    "\n",
    "                x_matrix[i:i+batch_size] = x_batch\n",
    "                y_matrix[i:i+batch_size] = y_batch\n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "    \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_lstm()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "        \n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_lstm()\n",
    "                continue\n",
    "            res = self.evaluate(test_x, test_y)\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "        \n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "        \n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_10653/2156677550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhist_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_n_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_10653/2588195815.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims, w2v_path, max_seq_len, batch_size, epochs, chunk_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_path  = 'data/original/cr/train.csv'\n",
    "test_path   = 'data/original/cr/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "name = 'cr'\n",
    "max_seq_len = 64\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "lstm = LSTM(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs)\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, n_classes = lstm.insert_values(train_path,test_path)\n",
    "hist_dict, res_dict, avg_dict = lstm.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, name, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, dims, w2v_path, max_seq_len=20, batch_size=128, epochs=20, chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:\n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None\n",
    "        self.callbacks = None\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "\n",
    "    def prepare_dataset(self, df):\n",
    "        def generator():\n",
    "            for _, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                x = np.zeros((self.max_seq_len, 300))\n",
    "                y = np.zeros(self.n_classes)\n",
    "\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for k, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x[k, :] = self.w2v[word]\n",
    "                y[label] = 1.0\n",
    "                yield x, y\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(self.max_seq_len, 300), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.n_classes,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def insert_values(self, train_path, test_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()\n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_dataset = self.prepare_dataset(train_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_dataset = self.prepare_dataset(test_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_dataset = self.prepare_dataset(val_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, test_dataset, val_dataset, self.n_classes\n",
    "\n",
    "    def fit(self, train_dataset, val_dataset):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()\n",
    "        self.history = self.model.fit(train_dataset, epochs=self.epochs, validation_data=val_dataset, callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        return self.model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "    def run_n_times(self, train_dataset, test_dataset, val_dataset, dataset_name, n=3):\n",
    "\n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_dataset, val_dataset)  # Updated to use train_dataset and val_dataset\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_cnn()\n",
    "                continue\n",
    "            res = self.evaluate(test_dataset)  # Updated to use test_dataset\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "\n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "\n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return hist_dict, res_dict, avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cardio'\n",
    "train_path = f'data/original/{name}/train.csv'\n",
    "test_path = f'data/original/{name}/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "dataset_name = f'{name}'\n",
    "max_seq_len = 64\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "cnn = CNN(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, chunk_size=1000)\n",
    "train_dataset, test_dataset, val_dataset, n_classes = cnn.insert_values(train_path, test_path)  # Updated to return datasets\n",
    "hist_dict, res_dict, avg_dict = cnn.run_n_times(train_dataset, test_dataset, val_dataset, name, n=3)  # Updated to use datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# = = = = =  = = = = = = =\n",
    "# L  S  T  M\n",
    "# = = = = = = = = = = = = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, dims, w2v_path, max_seq_len=20, batch_size=128, epochs=20, chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:\n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None\n",
    "        self.callbacks = None\n",
    "\n",
    "    def build_lstm(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        lstm_1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_layer)\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(lstm_1)\n",
    "        lstm_2 = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(dropout_out1)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(lstm_2)\n",
    "        dense_1 = layers.Dense(20, activation='relu')(dropout_out2)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dense_1)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        lstm_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        lstm_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #lstm_model.summary()\n",
    "        self.model = lstm_model\n",
    "\n",
    "    def prepare_dataset(self, df):\n",
    "        def generator():\n",
    "            for _, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                x = np.zeros((self.max_seq_len, 300))\n",
    "                y = np.zeros(self.n_classes)\n",
    "\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for k, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x[k, :] = self.w2v[word]\n",
    "                y[label] = 1.0\n",
    "                yield x, y\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(self.max_seq_len, 300), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.n_classes,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def insert_values(self, train_path, test_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()\n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_dataset = self.prepare_dataset(train_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_dataset = self.prepare_dataset(test_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_dataset = self.prepare_dataset(val_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, test_dataset, val_dataset, self.n_classes\n",
    "\n",
    "    def fit(self, train_dataset, val_dataset):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_lstm()\n",
    "        self.history = self.model.fit(train_dataset, epochs=self.epochs, validation_data=val_dataset, callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        return self.model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "    def run_n_times(self, train_dataset, test_dataset, val_dataset, dataset_name, n=3):\n",
    "\n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_dataset, val_dataset)  # Updated to use train_dataset and val_dataset\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_lstm()\n",
    "                continue\n",
    "            res = self.evaluate(test_dataset)  # Updated to use test_dataset\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/lstm/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "\n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "\n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results/lstm\", exist_ok=True)\n",
    "        with open(f\"results/lstm/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cr'\n",
    "train_path = f'data/original/{name}/train.csv'\n",
    "test_path = f'data/original/{name}/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "dataset_name = f'{name}'\n",
    "max_seq_len = 128\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "lstm = LSTM(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, chunk_size=1000)\n",
    "train_dataset, test_dataset, val_dataset, n_classes = lstm.insert_values(train_path, test_path)  # Updated to return datasets\n",
    "hist_dict, res_dict, avg_dict = lstm.run_n_times(train_dataset, test_dataset, val_dataset, name, n=3)  # Updated to use datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B   E   R   T\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/peymangcp1/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/peymangcp1/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/peymangcp1/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/peymangcp1/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/peymangcp1/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/peymangcp1/miniconda3/envs/train/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 545\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 105\n",
      "  Number of trainable parameters = 109486854\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 36/105 01:53 < 03:50, 0.30 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/8 00:15 < 00:02, 0.38 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.9643546938896179, 'eval_accuracy': 0.862, 'eval_f1': 0.8542543529009613, 'eval_precision': 0.8547603007894832, 'eval_recall': 0.862, 'eval_auc': 0.9560973989834445, 'eval_runtime': 18.0811, 'eval_samples_per_second': 27.653, 'eval_steps_per_second': 0.442, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peymangcp1/miniconda3/envs/train/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "train_path = 'data/original/trec/train.csv'\n",
    "test_path = 'data/original/trec/test.csv'\n",
    "# Load data\n",
    "train_data = pd.read_csv(train_path).sample(frac=0.1).reset_index(drop=True)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "# Remove rows with missing or invalid 'text' values\n",
    "train_data = train_data[train_data['text'].apply(lambda x: isinstance(x, str))]\n",
    "test_data = test_data[test_data['text'].apply(lambda x: isinstance(x, str))]\n",
    "    \n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train_data['class'].unique()))\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(train_data['text'].tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_data['text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Create dataset class\n",
    "class SimpleDataset:\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = SimpleDataset(train_encodings, train_data['class'].tolist())\n",
    "test_dataset = SimpleDataset(test_encodings, test_data['class'].tolist())\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = softmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    if len(np.unique(labels)) > 2:  # Multi-class case\n",
    "        auc = roc_auc_score(labels, probs, multi_class=\"ovo\", average=\"weighted\")\n",
    "    else:  # Binary case\n",
    "        auc = roc_auc_score(labels, probs[:, 1])  # Use the probability of the positive class\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "# Training and evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=0,\n",
    "    logging_dir='./logs/bert',\n",
    "    learning_rate=2e-5,\n",
    "   # gradient_accumulation_steps = 4\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "res = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred = np.argmax(res.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_predict, y_ground_truth):\n",
    "    assert len(y_predict) == len(y_ground_truth), \"Both lists must have the same length.\"\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for pred, gt in zip(y_predict, y_ground_truth):\n",
    "        if pred == gt:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(y_ground_truth) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.40%\n"
     ]
    }
   ],
   "source": [
    "y_ground_truth = test_data['class'].tolist()\n",
    "y_predict = pred\n",
    "accuracy = calculate_accuracy(pred, y_ground_truth)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 64\n",
      "/home/peymangcp1/miniconda3/envs/train/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "res = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy', 'f1', 'precision', 'recall', 'auc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.split('_')[-1] for idx,i in enumerate(list(res.keys())) if idx < 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9643546938896179,\n",
       " 'eval_accuracy': 0.862,\n",
       " 'eval_f1': 0.8542543529009613,\n",
       " 'eval_precision': 0.8547603007894832,\n",
       " 'eval_recall': 0.862,\n",
       " 'eval_auc': 0.9560973989834445,\n",
       " 'eval_runtime': 20.816,\n",
       " 'eval_samples_per_second': 24.02,\n",
       " 'eval_steps_per_second': 0.384,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "order = ['loss', 'auc', 'f1','accuracy']\n",
    "for i in res:\n",
    "    if i.split('_')[-1] in order:\n",
    "        key = i.split('_')[-1]\n",
    "        new_dict[key] = res[i]#.__format__('0.4f')\n",
    "df = pd.DataFrame(new_dict, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': '0.9644', 'accuracy': '0.8620', 'f1': '0.8543', 'auc': '0.9561'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9644</td>\n",
       "      <td>0.8620</td>\n",
       "      <td>0.8543</td>\n",
       "      <td>0.9561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss accuracy      f1     auc\n",
       "0  0.9644   0.8620  0.8543  0.9561"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "# Save the average results to disk\n",
    "import os\n",
    "os.makedirs(\"results/bert\", exist_ok=True)\n",
    "with open(f\"results/bert/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "    for key, value in avg_dict.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
