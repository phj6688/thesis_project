{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C  N  N\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "import random\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None        \n",
    "        self.label_mapping = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        log_dir = f\"logs/fit/run_only_once\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix = np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for j, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x_matrix[i, j, :] = self.w2v[word]\n",
    "                else:\n",
    "                    continue        \n",
    "                y_matrix[i,label] = 1.0    \n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "        \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "            hist_dict = {}\n",
    "            res_dict = {}\n",
    "            best_val_loss = float('inf')\n",
    "            for i in range(n):\n",
    "                print(f'Run {i+1} of {n}')\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "                res = self.evaluate(test_x, test_y)\n",
    "                res_dict[i+1] = res\n",
    "                if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                    best_val_loss = self.history.history['val_loss'][-1]\n",
    "                    self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "                self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "            \n",
    "            avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "            \n",
    "            # Save the average results to disk\n",
    "            os.makedirs(\"results\", exist_ok=True)\n",
    "            with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "                for key, value in avg_dict.items():\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            K.clear_session()\n",
    "            \n",
    "            return hist_dict, res_dict, avg_dict\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20,batch_size_insert=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_insert = batch_size_insert\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None        \n",
    "        self.label_mapping = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']        \n",
    "        self.callbacks =None\n",
    "        \n",
    "\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix =        np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i in range(0, num_lines, self.batch_size_insert):\n",
    "                df_batch = df.iloc[i:i+self.batch_size_insert]\n",
    "                batch_size = len(df_batch)\n",
    "                x_batch = np.zeros((batch_size, self.max_seq_len, 300))\n",
    "                y_batch = np.zeros((batch_size, self.n_classes))\n",
    "\n",
    "                for j, row in df_batch.iterrows():\n",
    "                    label = row[0]\n",
    "                    sentence = row[1]\n",
    "                    if isinstance(sentence, str):\n",
    "                        words = sentence.split()[:self.max_seq_len]\n",
    "                        for k, word in enumerate(words):\n",
    "                            if word in self.w2v:\n",
    "                                x_batch[j-i, k, :] = self.w2v[word]\n",
    "                    else:\n",
    "                        continue        \n",
    "                    y_batch[j-i,label] = 1.0\n",
    "\n",
    "                x_matrix[i:i+batch_size] = x_batch\n",
    "                y_matrix[i:i+batch_size] = y_batch\n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "    \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "        \n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_cnn()\n",
    "                continue\n",
    "            res = self.evaluate(test_x, test_y)\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "        \n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "        \n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'data/original/agnews/train.csv'\n",
    "test_path   = 'data/original/agnews/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "name = 'agnews'\n",
    "max_seq_len = 150\n",
    "batch_size = 8\n",
    "epochs = 30\n",
    "cnn = CNN(dims=300, w2v_path=w2v_path, max_seq_len=20, batch_size=128, epochs=20)\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, n_classes = cnn.insert_values(train_path,test_path)\n",
    "hist_dict, res_dict, avg_dict = cnn.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, name, n=3)\n",
    "\n",
    "\n",
    "# model = CNN(dims=300, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, w2v_path=w2v_path)\n",
    "# train_x, train_y, test_x, test_y, val_x, val_y, n_classes = model.insert_values(train_path,test_path)\n",
    "# his,res,avg = model.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, n=3)\n",
    "# print (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "import random\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20,chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None                \n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']        \n",
    "        self.callbacks =None\n",
    "        \n",
    "\n",
    "    def build_lstm(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        lstm_1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_layer)\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(lstm_1)\n",
    "        lstm_2 = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(dropout_out1)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(lstm_2)\n",
    "        dense_1 = layers.Dense(20, activation='relu')(dropout_out2)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dense_1)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        lstm_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        lstm_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #lstm_model.summary()\n",
    "        self.model = lstm_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix = np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i in range(0, num_lines, self.chunk_size):\n",
    "                df_batch = df.iloc[i:i+self.chunk_size]\n",
    "                batch_size = len(df_batch)\n",
    "                x_batch = np.zeros((batch_size, self.max_seq_len, 300))\n",
    "                y_batch = np.zeros((batch_size, self.n_classes))\n",
    "\n",
    "                for j, row in df_batch.iterrows():\n",
    "                    label = row[0]\n",
    "                    sentence = row[1]\n",
    "                    if isinstance(sentence, str):\n",
    "                        words = sentence.split()[:self.max_seq_len]\n",
    "                        for k, word in enumerate(words):\n",
    "                            if word in self.w2v:\n",
    "                                x_batch[j-i, k, :] = self.w2v[word]\n",
    "                    else:\n",
    "                        continue        \n",
    "                    y_batch[j-i,label] = 1.0\n",
    "\n",
    "                x_matrix[i:i+batch_size] = x_batch\n",
    "                y_matrix[i:i+batch_size] = y_batch\n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "    \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_lstm()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "        \n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_lstm()\n",
    "                continue\n",
    "            res = self.evaluate(test_x, test_y)\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "        \n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "        \n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'data/original/cr/train.csv'\n",
    "test_path   = 'data/original/cr/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "name = 'cr'\n",
    "max_seq_len = 64\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "lstm = LSTM(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs)\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, n_classes = lstm.insert_values(train_path,test_path)\n",
    "hist_dict, res_dict, avg_dict = lstm.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, name, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, dims, w2v_path, max_seq_len=20, batch_size=128, epochs=20, chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:\n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None\n",
    "        self.callbacks = None\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "\n",
    "    def prepare_dataset(self, df):\n",
    "        def generator():\n",
    "            for _, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                x = np.zeros((self.max_seq_len, 300))\n",
    "                y = np.zeros(self.n_classes)\n",
    "\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for k, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x[k, :] = self.w2v[word]\n",
    "                y[label] = 1.0\n",
    "                yield x, y\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(self.max_seq_len, 300), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.n_classes,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def insert_values(self, train_path, test_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()\n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_dataset = self.prepare_dataset(train_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_dataset = self.prepare_dataset(test_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_dataset = self.prepare_dataset(val_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, test_dataset, val_dataset, self.n_classes\n",
    "\n",
    "    def fit(self, train_dataset, val_dataset):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()\n",
    "        self.history = self.model.fit(train_dataset, epochs=self.epochs, validation_data=val_dataset, callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        return self.model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "    def run_n_times(self, train_dataset, test_dataset, val_dataset, dataset_name, n=3):\n",
    "\n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_dataset, val_dataset)  # Updated to use train_dataset and val_dataset\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_cnn()\n",
    "                continue\n",
    "            res = self.evaluate(test_dataset)  # Updated to use test_dataset\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "\n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "\n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return hist_dict, res_dict, avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cardio'\n",
    "train_path = f'data/original/{name}/train.csv'\n",
    "test_path = f'data/original/{name}/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "dataset_name = f'{name}'\n",
    "max_seq_len = 64\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "cnn = CNN(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, chunk_size=1000)\n",
    "train_dataset, test_dataset, val_dataset, n_classes = cnn.insert_values(train_path, test_path)  # Updated to return datasets\n",
    "hist_dict, res_dict, avg_dict = cnn.run_n_times(train_dataset, test_dataset, val_dataset, name, n=3)  # Updated to use datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# = = = = =  = = = = = = =\n",
    "# L  S  T  M\n",
    "# = = = = = = = = = = = = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, dims, w2v_path, max_seq_len=20, batch_size=128, epochs=20, chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:\n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None\n",
    "        self.callbacks = None\n",
    "\n",
    "    def build_lstm(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        lstm_1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_layer)\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(lstm_1)\n",
    "        lstm_2 = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(dropout_out1)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(lstm_2)\n",
    "        dense_1 = layers.Dense(20, activation='relu')(dropout_out2)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dense_1)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        lstm_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        lstm_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #lstm_model.summary()\n",
    "        self.model = lstm_model\n",
    "\n",
    "    def prepare_dataset(self, df):\n",
    "        def generator():\n",
    "            for _, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                x = np.zeros((self.max_seq_len, 300))\n",
    "                y = np.zeros(self.n_classes)\n",
    "\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for k, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x[k, :] = self.w2v[word]\n",
    "                y[label] = 1.0\n",
    "                yield x, y\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(self.max_seq_len, 300), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.n_classes,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def insert_values(self, train_path, test_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()\n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_dataset = self.prepare_dataset(train_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_dataset = self.prepare_dataset(test_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_dataset = self.prepare_dataset(val_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, test_dataset, val_dataset, self.n_classes\n",
    "\n",
    "    def fit(self, train_dataset, val_dataset):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_lstm()\n",
    "        self.history = self.model.fit(train_dataset, epochs=self.epochs, validation_data=val_dataset, callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        return self.model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "    def run_n_times(self, train_dataset, test_dataset, val_dataset, dataset_name, n=3):\n",
    "\n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_dataset, val_dataset)  # Updated to use train_dataset and val_dataset\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_lstm()\n",
    "                continue\n",
    "            res = self.evaluate(test_dataset)  # Updated to use test_dataset\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/lstm/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "\n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "\n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results/lstm\", exist_ok=True)\n",
    "        with open(f\"results/lstm/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cr'\n",
    "train_path = f'data/original/{name}/train.csv'\n",
    "test_path = f'data/original/{name}/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "dataset_name = f'{name}'\n",
    "max_seq_len = 128\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "lstm = LSTM(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, chunk_size=1000)\n",
    "train_dataset, test_dataset, val_dataset, n_classes = lstm.insert_values(train_path, test_path)  # Updated to return datasets\n",
    "hist_dict, res_dict, avg_dict = lstm.run_n_times(train_dataset, test_dataset, val_dataset, name, n=3)  # Updated to use datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B   E   R   T\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "import os\n",
    "# disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "train_path = 'data/original/cr/train.csv'\n",
    "test_path = 'data/original/cr/test.csv'\n",
    "# Load data\n",
    "train_data = pd.read_csv(train_path).sample(frac=1).reset_index(drop=True)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_data['class'] = encoder.fit_transform(train_data['class'])\n",
    "test_data['class'] = encoder.transform(test_data['class'])\n",
    "# Remove rows with missing or invalid 'text' values\n",
    "train_data = train_data[train_data['text'].apply(lambda x: isinstance(x, str))]\n",
    "test_data = test_data[test_data['text'].apply(lambda x: isinstance(x, str))]\n",
    "    \n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train_data['class'].unique()))\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(train_data['text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_data['text'].tolist(), truncation=True, padding=True,max_length=512)\n",
    "\n",
    "# Create dataset class\n",
    "class SimpleDataset:\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = SimpleDataset(train_encodings, train_data['class'].tolist())\n",
    "test_dataset = SimpleDataset(test_encodings, test_data['class'].tolist())\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = softmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    if len(np.unique(labels)) > 2:  # Multi-class case\n",
    "        auc = roc_auc_score(labels, probs, multi_class=\"ovo\", average=\"weighted\")\n",
    "    else:  # Binary case\n",
    "        auc = roc_auc_score(labels, probs[:, 1])  # Use the probability of the positive class\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "# Training and evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=0,\n",
    "    logging_dir='./logs/bert',\n",
    "    learning_rate=2e-5,\n",
    "    #fp16=True,\n",
    "   gradient_accumulation_steps = 8\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred = np.argmax(res.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_predict, y_ground_truth):\n",
    "    assert len(y_predict) == len(y_ground_truth), \"Both lists must have the same length.\"\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for pred, gt in zip(y_predict, y_ground_truth):\n",
    "        if pred == gt:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(y_ground_truth) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ground_truth = test_data['class'].tolist()\n",
    "y_predict = pred\n",
    "accuracy = calculate_accuracy(pred, y_ground_truth)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.split('_')[-1] for idx,i in enumerate(list(res.keys())) if idx < 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "order = ['loss', 'auc', 'f1','accuracy']\n",
    "for i in res:\n",
    "    if i.split('_')[-1] in order:\n",
    "        key = i.split('_')[-1]\n",
    "        new_dict[key] = res[i]#.__format__('0.4f')\n",
    "df = pd.DataFrame(new_dict, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "# Save the average results to disk\n",
    "import os\n",
    "os.makedirs(\"results/bert\", exist_ok=True)\n",
    "with open(f\"results/bert/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "    for key, value in avg_dict.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_path).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['class'].nunique(), len(train_data['class'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========Bert V2 ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast ,DistilBertForSequenceClassification# AdamW\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from numpy import mean\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/original/cr/train.csv'\n",
    "test_path = 'data/original/cr/test.csv'\n",
    "# Load data\n",
    "train_data = pd.read_csv(train_path).sample(frac=1).reset_index(drop=True)\n",
    "test_data = pd.read_csv(test_path).sample(frac=1).reset_index(drop=True)\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['class'].tolist()\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['class'].tolist()\n",
    "\n",
    "print(f\"Number of training examples: {len(train_texts)}\")\n",
    "print(f\"Number of test examples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "num_of_epochs = 3\n",
    "learning_rate = 10e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_encodings, train_labels)\n",
    "test_dataset = Dataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    size = len(data_loader.dataset)\t\n",
    "    for i,batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Train loss: {epoch_loss/size:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader, model):\n",
    "    model.eval()\n",
    "    size = len(data_loader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            X, y = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            pred = model(X,labels=y)\n",
    "            test_loss += pred.loss\n",
    "            accuracy += (pred.logits.softmax(1).argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= size\n",
    "        accuracy /= size\n",
    "        print(f\"Test loss: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.write(\"Training the model...\")\n",
    "tqdm.pandas()\n",
    "for i in tqdm(range(num_of_epochs)):\n",
    "    print(f'Epoch {i+1}')\n",
    "    train(train_loader, model, optimizer)\n",
    "    test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "train_path = 'data/original/cr/train.csv'\n",
    "test_path = 'data/original/cr/test.csv'\n",
    "# Load data\n",
    "train_data = pd.read_csv(train_path).sample(frac=1).reset_index(drop=True)\n",
    "test_data = pd.read_csv(test_path).sample(frac=1).reset_index(drop=True)\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['class'].tolist()\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['class'].tolist()\n",
    "\n",
    "print(f\"Number of training examples: {len(train_texts)}\")\n",
    "print(f\"Number of test examples: {len(test_texts)}\")\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Define model and trainer\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(train_labels)))\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, p.predictions.argmax(-1))}\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== BERT Combined ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self, model_name, train_path, test_path, training_args):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.train_dataset, self.test_dataset, self.n_classes = self.prepare_dataset(train_path, test_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=self.n_classes)\n",
    "        self.training_args = training_args\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.test_dataset,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self, train_path, test_path):\n",
    "        train_data = pd.read_csv(train_path).sample(frac=1).reset_index(drop=True)\n",
    "        test_data = pd.read_csv(test_path).sample(frac=1).reset_index(drop=True)\n",
    "        encoder = LabelEncoder()\n",
    "        train_data['class'] = encoder.fit_transform(train_data['class'])\n",
    "        test_data['class'] = encoder.transform(test_data['class'])\n",
    "\n",
    "        train_encodings = self.tokenizer(train_data['text'].tolist(), truncation=True, padding=True)\n",
    "        test_encodings = self.tokenizer(test_data['text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "        train_dataset = TextDataset(train_encodings, train_data['class'].tolist())\n",
    "        test_dataset = TextDataset(test_encodings, test_data['class'].tolist())\n",
    "        n_classes = len(train_data['class'].unique())\n",
    "\n",
    "        return train_dataset, test_dataset, n_classes\n",
    "\n",
    "    def compute_metrics(self, pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        probs = softmax(pred.predictions, axis=1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        auc = roc_auc_score(labels, probs, multi_class=\"ovo\", average=\"weighted\") if self.n_classes > 2 else roc_auc_score(labels, probs[:, 1])\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"auc\": auc\n",
    "        }\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        self.trainer.train()\n",
    "        eval_results = self.trainer.evaluate()\n",
    "        return eval_results\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "train_path = 'data/original/cr/train.csv'\n",
    "test_path = 'data/original/cr/test.csv'\n",
    "\n",
    "classifier = TextClassifier(\n",
    "    model_name='distilbert-base-uncased',\n",
    "    train_path=train_path,\n",
    "    test_path=test_path,\n",
    "    training_args=training_args\n",
    ")\n",
    "\n",
    "eval_results = classifier.train_and_evaluate()\n",
    "print(eval_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#set seed to 100 \n",
    "np.random.seed(100)\n",
    "\n",
    "\n",
    "\n",
    "# disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class MyTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return self.train_dataloader\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        return self.eval_dataloader\n",
    "\n",
    "class BERT:\n",
    "    def __init__(self, train_path, test_path, trainings_arguments: TrainingArguments, model_name='distilbert-base-uncased'):\n",
    "        # Define collate_fn\n",
    "        def collate_fn(batch):\n",
    "            keys = batch[0].keys()\n",
    "            output_batch = {}\n",
    "            for key in keys:\n",
    "                items = [item[key] for item in batch]\n",
    "                if isinstance(items[0], torch.Tensor):\n",
    "                    output_batch[key] = torch.stack(items)\n",
    "                else:\n",
    "                    output_batch[key] = torch.tensor(items)\n",
    "            return output_batch\n",
    "\n",
    "\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.n_runs = None\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.trainings_arguments = trainings_arguments        \n",
    "        self.train_dataset, self.val_dataset, self.test_dataset, self.n_classes = self.prepare_dataset(train_path, test_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=self.n_classes)\n",
    "        self.compute_metrics_func = self.compute_metrics\n",
    "        \n",
    "        # Create the DataLoaders\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=self.trainings_arguments.per_device_train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=self.trainings_arguments.per_device_eval_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=self.trainings_arguments.per_device_eval_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        \n",
    "        self.trainer = MyTrainer(\n",
    "            model=self.model,\n",
    "            args=self.trainings_arguments,\n",
    "            compute_metrics=self.compute_metrics_func)\n",
    "        \n",
    "        self.trainer.train_dataloader = self.train_dataloader\n",
    "        self.trainer.eval_dataloader = self.val_dataloader\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "    def prepare_dataset(self, train_path, test_path):\n",
    "        train_data = pd.read_csv(train_path).sample(frac=1).reset_index(drop=True)\n",
    "        test_data = pd.read_csv(test_path).sample(frac=1).reset_index(drop=True)\n",
    "        n_classes = len(train_data['class'].unique())        \n",
    "        # encode the labels\n",
    "        encoder = LabelEncoder()\n",
    "        train_data['class'] = encoder.fit_transform(train_data['class'])\n",
    "        test_data['class'] = encoder.transform(test_data['class'])\n",
    "        # Remove rows with missing or invalid 'text' values\n",
    "        train_data = train_data[train_data['text'].apply(lambda x: isinstance(x, str))]\n",
    "        test_data = test_data[test_data['text'].apply(lambda x: isinstance(x, str))]        \n",
    "        \n",
    "        # Split train_data into train and validation sets\n",
    "        train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42, stratify=train_data['class'])\n",
    "\n",
    "        # tokenize the text\n",
    "        train_encodings = self.tokenizer(train_data['text'].tolist(), truncation=True, padding=True)\n",
    "        val_encodings = self.tokenizer(val_data['text'].tolist(), truncation=True, padding=True)\n",
    "        test_encodings = self.tokenizer(test_data['text'].tolist(), truncation=True, padding=True)\n",
    "        # create dataset\n",
    "        train_dataset = CustomDataset(train_encodings, train_data['class'].tolist())\n",
    "        val_dataset = CustomDataset(val_encodings, val_data['class'].tolist())\n",
    "        test_dataset = CustomDataset(test_encodings, test_data['class'].tolist())\n",
    "        return train_dataset, val_dataset, test_dataset, n_classes\n",
    "\n",
    "\n",
    "    def compute_metrics(self,pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        probs = softmax(pred.predictions, axis=1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        if len(np.unique(labels)) > 2:  # Multi-class case\n",
    "            auc = roc_auc_score(labels, probs, multi_class=\"ovo\", average=\"weighted\")\n",
    "        else:  # Binary case\n",
    "            auc = roc_auc_score(labels, probs[:, 1])  # Use the probability of the positive class\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"auc\": auc\n",
    "        }\n",
    "    \n",
    "    def train_and_evaluate(self, run_idx, dataset_name):\n",
    "        print(f'Run {run_idx} of {self.n_runs}')\n",
    "        # Load initial model state\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=self.n_classes)\n",
    "        self.trainer.model = self.model\n",
    "        self.trainer.train()\n",
    "        self.trainer.save_model(os.path.join(\"models\", \"bert\", f\"{dataset_name}_run_{run_idx}_model\"))\n",
    "        results = self.trainer.evaluate()\n",
    "        if results['eval_loss'] < self.best_val_loss:\n",
    "            self.best_val_loss = results['eval_loss']\n",
    "            self.trainer.save_model(os.path.join(\"models\", \"bert\", f\"{dataset_name}_best_model\"))\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "    def clean_up_models(self, dataset_name):\n",
    "        for i in range(self.n_runs):\n",
    "            shutil.rmtree(f\"models/bert/{dataset_name}_run_{i+1}_model\")\n",
    "\n",
    "    def calculate_and_save_averages(self, res_dict, dataset_name):\n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "        order = ['loss', 'auc', 'f1', 'accuracy']\n",
    "        filtered_metrics = {i: avg_dict[f\"eval_{i}\"] for i in order if f\"eval_{i}\" in avg_dict}\n",
    "        os.makedirs(\"results/bert\", exist_ok=True)\n",
    "        with open(f\"results/bert/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in filtered_metrics.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        return filtered_metrics\n",
    "\n",
    "\n",
    "    def run_n_times(self, dataset_name, n=3):   \n",
    "        self.n_runs = n\n",
    "        self.best_val_loss = float('inf')\n",
    "        res_dict = {}\n",
    "\n",
    "        for i in range(n):\n",
    "            res_dict[i+1] = self.train_and_evaluate(i+1, dataset_name)\n",
    "            \n",
    "        self.clean_up_models(dataset_name)\n",
    "        avg_metrics = self.calculate_and_save_averages(res_dict, dataset_name)\n",
    "\n",
    "        return avg_metrics\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "#dataset_list = ['trec','agnews', 'pc', 'yelp', 'cr', 'kaggle_med', 'cardio', 'bbc', 'sst2','subj']\n",
    "dataset_list = ['cr']\n",
    "\n",
    "\n",
    "for name in dataset_list:\n",
    "    try:\n",
    "        print(f'Running {name} dataset')\n",
    "        train_path  = f'data/original/{name}/train.csv'\n",
    "        test_path   = f'data/original/{name}/test.csv'\n",
    "        model_name = 'distilbert-base-uncased'\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./models/bert',\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            # eval_steps=100,\n",
    "            logging_steps=10,\n",
    "            # save_steps=0,\n",
    "            logging_dir='./logs/bert',\n",
    "            metric_for_best_model=\"f1\",\n",
    "            #learning_rate=2e-5,\n",
    "            seed=100\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        bert = BERT(train_path, test_path, training_args, model_name=model_name)            \n",
    "        avg_dict = bert.run_n_times(name, n=3)\n",
    "        print('---------------------------------------------------')\n",
    "        print(f'Average results for {name} dataset')\n",
    "        print(avg_dict)\n",
    "        print('---------------------------------------------------')\n",
    "    except Exception as e:\n",
    "        print(f'Error in {name}')\n",
    "        print(str(e))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = f'data/original/cr/train.csv'\n",
    "test_path   = f'data/original/cr/test.csv'\n",
    "model_name = 'distilbert-base-uncased'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/bert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    # save_steps=0,\n",
    "    logging_dir='./logs/bert',\n",
    "    metric_for_best_model=\"f1\",\n",
    "    learning_rate=2e-5,\n",
    "    seed=100\n",
    "    )\n",
    "\n",
    "bert = BERT(train_path, test_path, training_args, model_name=model_name)\n",
    "# avg_dict = bert.run_n_times(name, n=3)\n",
    "# print('---------------------------------------------------')\n",
    "# print(f'Average results for {name} dataset')\n",
    "# print(avg_dict)\n",
    "# print('---------------------------------------------------')\n",
    "res = bert.train_and_evaluate(1, 'cr')\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== Simple Bert ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "class SimpleBert:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.args = {\n",
    "        'output_dir': f'./models/bert/{self.dataset_name}',\n",
    "        'reprocess_input_data': True,\n",
    "        'overwrite_output_dir': True,\n",
    "        'train_batch_size': 8,\n",
    "        'num_train_epochs': 3,\n",
    "        'use_multiprocessing': False,\n",
    "        'use_multiprocessing_for_evaluation': False,\n",
    "        }\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        self.model = None\n",
    "        self.result = None\n",
    "        self.num_labels = None\n",
    "        self.result_metrics = None\n",
    "\n",
    "    def load_data(self):\n",
    "        encoder = LabelEncoder()\n",
    "        self.train = pd.read_csv(f'data/original/{self.dataset_name}/train.csv').sample(frac=0.1) # shuffle\n",
    "        self.test = pd.read_csv(f'data/original/{self.dataset_name}/test.csv')\n",
    "        self.train = self.train[['text', 'class']]\n",
    "        self.test = self.test[['text', 'class']]\n",
    "        self.train.columns = ['text', 'labels']\n",
    "        self.test.columns = ['text', 'labels']\n",
    "        self.train['labels'] = encoder.fit_transform(self.train['labels']) # encode the labels to start from 0\n",
    "        self.test['labels'] = encoder.transform(self.test['labels'])\n",
    "        self.num_labels = self.train['labels'].nunique()\n",
    "\n",
    "    def train_model(self):        \n",
    "        self.model = ClassificationModel('distilbert', 'distilbert-base-uncased', num_labels=self.num_labels, cuda_device=0, use_cuda=True, args=self.args)\n",
    "        self.model.train_model(self.train)\n",
    "\n",
    "    def compute_metrics(self, preds, labels):\n",
    "        self.result_metrics\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        prec = precision_score(labels, preds, average='macro')\n",
    "        rec = recall_score(labels, preds, average='macro')\n",
    "        f1 = f1_score(labels, preds, average='macro')\n",
    "\n",
    "        return {\n",
    "            'acc': acc,\n",
    "            'f1': f1,\n",
    "            'prec': prec,\n",
    "            'rec': rec\n",
    "        }\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        self.result, _, _ = self.model.eval_model(self.test, compute_metrics=self.compute_metrics)\n",
    "        self.result_metrics = self.result\n",
    "        return self.result_metrics\n",
    "\n",
    "    # def save_model(self, output_dir):\n",
    "    #     self.model.save_model(output_dir)\n",
    "\n",
    "    def save_results(self, output_file):\n",
    "        with open(output_file, 'w') as f:\n",
    "            if 'compute_metrics' in self.result_metrics:\n",
    "                metrics = self.result_metrics['compute_metrics']\n",
    "                for key, value in metrics.items():\n",
    "                    f.write(f\"{key}: {round(value,4)}\\n\")\n",
    "            for key, value in self.result_metrics.items():\n",
    "                if key != 'compute_metrics':\n",
    "                    f.write(f\"{key}: {round(value,4)}\\n\")\n",
    "\n",
    "    def clean_up(self):\n",
    "        #remove all checkpoints folders \n",
    "        checkpoints_folder = f'./models/bert/{self.dataset_name}'\n",
    "        pattern = f'{checkpoints_folder}/checkpoint-*'\n",
    "\n",
    "        for folder in glob.glob(pattern):\n",
    "            shutil.rmtree(folder)\n",
    "\n",
    "    def extract_pre_last_layer(self, text):\n",
    "        # Tokenize the input text\n",
    "        tokenizer = self.model.tokenizer\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        # Move input tensors to the same device as the model (CPU or GPU)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Get the output from the base model (layer before the last layer)\n",
    "        with torch.no_grad():\n",
    "            base_model_output = self.model.model.distilbert(**inputs)\n",
    "\n",
    "        hidden_states = base_model_output.last_hidden_state\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trec'\n",
    "print(f'Running {dataset} dataset')\n",
    "simple_bert = SimpleBert(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded data for {dataset} dataset\")\n",
    "simple_bert.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trained model for {dataset} dataset\")\n",
    "simple_bert.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluated model for {dataset} dataset\")\n",
    "res = simple_bert.evaluate_model()\n",
    "print(f'results: \\n\\n\\n{res}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#simple_bert.save_model()\n",
    "simple_bert.save_results(f\"results/bert/{dataset}_result.txt\")\n",
    "print(f\"Saved model and results for {dataset} dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_bert.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "class SimpleBert:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.args = {\n",
    "        'output_dir': f'./models/bert/{self.dataset_name}',\n",
    "        'reprocess_input_data': True,\n",
    "        'overwrite_output_dir': True,\n",
    "        'train_batch_size': 16,\n",
    "        'num_train_epochs': 4,\n",
    "        #'learning_rate': 2e-5,\n",
    "        'use_multiprocessing': False,\n",
    "        'use_multiprocessing_for_evaluation': False,\n",
    "        #'weight_decay': 0.01,\n",
    "        #'warmup_steps': 500,\n",
    "        'gradient_accumulation_steps': 2,        \n",
    "        }\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        self.model = None\n",
    "        self.result = None\n",
    "        self.num_labels = None\n",
    "        self.result_metrics = None\n",
    "        self.n_times_results = None\n",
    "\n",
    "    def load_data(self):\n",
    "        encoder = LabelEncoder()\n",
    "        self.train = pd.read_csv(f'data/original/{self.dataset_name}/train.csv').sample(frac=0.1) # shuffle\n",
    "        self.test = pd.read_csv(f'data/original/{self.dataset_name}/test.csv')\n",
    "        self.train = self.train[['text', 'class']]\n",
    "        self.test = self.test[['text', 'class']]\n",
    "        self.train.columns = ['text', 'labels']\n",
    "        self.test.columns = ['text', 'labels']\n",
    "        self.train['labels'] = encoder.fit_transform(self.train['labels']) # encode the labels to start from 0\n",
    "        self.test['labels'] = encoder.transform(self.test['labels'])\n",
    "        self.num_labels = self.train['labels'].nunique()\n",
    "\n",
    "    def train_model(self):        \n",
    "        self.model = ClassificationModel('distilbert', 'distilbert-base-uncased', num_labels=self.num_labels, cuda_device=0, use_cuda=True, args=self.args)\n",
    "        self.model.train_model(self.train)\n",
    "\n",
    "    def compute_metrics(self, preds, labels):        \n",
    "        if self.num_labels == 2:\n",
    "            average = 'binary'\n",
    "        else:\n",
    "            average = 'macro'\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        prec = precision_score(labels, preds, average=average)\n",
    "        rec = recall_score(labels, preds, average=average)\n",
    "        f1 = f1_score(labels, preds, average=average)\n",
    "\n",
    "        return {\n",
    "            'acc': acc,\n",
    "            'f1': f1,\n",
    "            'prec': prec,\n",
    "            'rec': rec\n",
    "        }\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        self.result, _, _ = self.model.eval_model(self.test, compute_metrics=self.compute_metrics)\n",
    "        self.result_metrics = self.result\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.result_metrics\n",
    "\n",
    "\n",
    "    def run_n_times(self, n):\n",
    "        dict_list = []                \n",
    "        for i in range(n):\n",
    "            self.model = None            \n",
    "            self.model = ClassificationModel('distilbert', 'distilbert-base-uncased', num_labels=self.num_labels, cuda_device=0, use_cuda=True, args=self.args)\n",
    "            self.train_model()\n",
    "            self.evaluate_model()\n",
    "            metrics = self.save_results(write_to_file=False)\n",
    "\n",
    "            temp_dict = {}\n",
    "            for key, value in metrics.items():         \n",
    "                temp_dict[key] = value\n",
    "            dict_list.append(temp_dict)\n",
    "        \n",
    "        new_dict = {}\n",
    "        for i in range(n):\n",
    "            for key, value in dict_list[i].items():\n",
    "                if key in new_dict:\n",
    "                    new_dict[key] += value\n",
    "                else:\n",
    "                    new_dict[key] = value\n",
    "                    \n",
    "        new_dict = {key: round(value/n,4) for key, value in new_dict.items()}\n",
    "\n",
    "        self.n_times_results = new_dict\n",
    "        return new_dict\n",
    "\n",
    "\n",
    "    def save_results(self, output_file=None, write_to_file=False, n_times=False):\n",
    "\n",
    "        def format_metrics(metrics):\n",
    "            return \"\\n\".join(f\"{key}: {round(value, 4)}\" for key, value in metrics.items())\n",
    "\n",
    "        if n_times:\n",
    "            metrics = self.n_times_results\n",
    "        elif 'compute_metrics' in self.result_metrics:\n",
    "            metrics = self.result_metrics['compute_metrics']\n",
    "        else:\n",
    "            metrics = self.result_metrics\n",
    "\n",
    "        result_str = format_metrics(metrics)\n",
    "\n",
    "        if write_to_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(result_str)\n",
    "        round_metrics = {key: round(value, 4) for key, value in metrics.items()}\n",
    "        return round_metrics\n",
    "\n",
    "\n",
    "\n",
    "    def clean_up(self):\n",
    "        #remove all checkpoints folders \n",
    "        checkpoints_folder = f'./models/bert/{self.dataset_name}'\n",
    "        pattern = f'{checkpoints_folder}/checkpoint-*'\n",
    "\n",
    "        for folder in glob.glob(pattern):\n",
    "            shutil.rmtree(folder)\n",
    "\n",
    "    def extract_pre_last_layer(self, text):\n",
    "        # Tokenize the input text\n",
    "        tokenizer = self.model.tokenizer\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        # Move input tensors to the same device as the model (CPU or GPU)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Get the output from the base model (layer before the last layer)\n",
    "        with torch.no_grad():\n",
    "            base_model_output = self.model.model.distilbert(**inputs)\n",
    "\n",
    "        hidden_states = base_model_output.last_hidden_state\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011911630630493164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac26ffbf005413f80d19bee3836a10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011523962020874023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 0 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1deeb6e5d6489d9fc30b7e79de0d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012609720230102539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 1 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e97f431f98e4fe9995b54df4465d37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012403726577758789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 2 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c0d8728218483bacc90c73472b3480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012609720230102539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 3 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aafeb94464a4412a27f220311da7390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011990785598754883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Evaluation",
       "rate": null,
       "total": 125,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dc83ff7e6142d2af5c5a173dd5411f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012061595916748047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ac0fa0357046eeac287973cea2fc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012185335159301758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 0 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959261528b024416a8c136aa175936fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01232290267944336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 1 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c346549892bd40b0b98e85ab9232b596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012537479400634766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 2 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b5e1fda1524729bbbfedb360227da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023293733596801758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 3 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b4bb44c9e149fdb1b2e79ed1b73ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012999534606933594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Evaluation",
       "rate": null,
       "total": 125,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd783ca14e74852979541c2710490e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011722326278686523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3e058d53514afc8fadc51f29ec9e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011165142059326172,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 0 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1666c6de3642b78a88e5ac25c8d335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012634038925170898,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 1 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7cc94098a24105bfb8b96c48645cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.035130977630615234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 2 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c96d5f8903b42de972e7ea7d24e2598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012618064880371094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 3 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab1eb5b75c7498fb2fada652dde6589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011695623397827148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Evaluation",
       "rate": null,
       "total": 125,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05110173d73e40e5bdc475e8a24c2a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011877775192260742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2926ee839a04a1695850833f7b6b4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011566877365112305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 0 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd97b907563140338e15f13835332b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012674093246459961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 1 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4cf1e47ef34f829e6866ad609c2842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012483835220336914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 2 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef10de91ec747d990d4f8add26fb9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012928485870361328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Epoch 3 of 4",
       "rate": null,
       "total": 57,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1392ebc1e949858f7e599db5aadf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 4:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01222372055053711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running Evaluation",
       "rate": null,
       "total": 125,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddad0fbd25a4e0a8b7aa9db04d1ae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dataset = 'yelp'\n",
    "simple_bert = SimpleBert(dataset)\n",
    "simple_bert.load_data()\n",
    "simple_bert.train_model()\n",
    "simple_bert.evaluate_model()\n",
    "n3 = simple_bert.run_n_times(3)\n",
    "res = simple_bert.save_results(n_times=True, write_to_file=True, output_file=f'./results/bert/{dataset}_n3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.486, 'f1': 0.3786, 'prec': 0.3841, 'rec': 0.405}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.486, 'f1': 0.3786, 'prec': 0.3841, 'rec': 0.405}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
