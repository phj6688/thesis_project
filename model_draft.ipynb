{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C  N  N\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "import random\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None        \n",
    "        self.label_mapping = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        log_dir = f\"logs/fit/run_only_once\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix = np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for j, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x_matrix[i, j, :] = self.w2v[word]\n",
    "                else:\n",
    "                    continue        \n",
    "                y_matrix[i,label] = 1.0    \n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "        \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "            hist_dict = {}\n",
    "            res_dict = {}\n",
    "            best_val_loss = float('inf')\n",
    "            for i in range(n):\n",
    "                print(f'Run {i+1} of {n}')\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "                res = self.evaluate(test_x, test_y)\n",
    "                res_dict[i+1] = res\n",
    "                if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                    best_val_loss = self.history.history['val_loss'][-1]\n",
    "                    self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "                self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "            \n",
    "            avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "            \n",
    "            # Save the average results to disk\n",
    "            os.makedirs(\"results\", exist_ok=True)\n",
    "            with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "                for key, value in avg_dict.items():\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "            \n",
    "            K.clear_session()\n",
    "            \n",
    "            return hist_dict, res_dict, avg_dict\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20,batch_size_insert=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_insert = batch_size_insert\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None        \n",
    "        self.label_mapping = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']        \n",
    "        self.callbacks =None\n",
    "        \n",
    "\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix =        np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i in range(0, num_lines, self.batch_size_insert):\n",
    "                df_batch = df.iloc[i:i+self.batch_size_insert]\n",
    "                batch_size = len(df_batch)\n",
    "                x_batch = np.zeros((batch_size, self.max_seq_len, 300))\n",
    "                y_batch = np.zeros((batch_size, self.n_classes))\n",
    "\n",
    "                for j, row in df_batch.iterrows():\n",
    "                    label = row[0]\n",
    "                    sentence = row[1]\n",
    "                    if isinstance(sentence, str):\n",
    "                        words = sentence.split()[:self.max_seq_len]\n",
    "                        for k, word in enumerate(words):\n",
    "                            if word in self.w2v:\n",
    "                                x_batch[j-i, k, :] = self.w2v[word]\n",
    "                    else:\n",
    "                        continue        \n",
    "                    y_batch[j-i,label] = 1.0\n",
    "\n",
    "                x_matrix[i:i+batch_size] = x_batch\n",
    "                y_matrix[i:i+batch_size] = y_batch\n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "    \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "        \n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_cnn()\n",
    "                continue\n",
    "            res = self.evaluate(test_x, test_y)\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "        \n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "        \n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'data/original/agnews/train.csv'\n",
    "test_path   = 'data/original/agnews/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "name = 'agnews'\n",
    "max_seq_len = 150\n",
    "batch_size = 8\n",
    "epochs = 30\n",
    "cnn = CNN(dims=300, w2v_path=w2v_path, max_seq_len=20, batch_size=128, epochs=20)\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, n_classes = cnn.insert_values(train_path,test_path)\n",
    "hist_dict, res_dict, avg_dict = cnn.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, name, n=3)\n",
    "\n",
    "\n",
    "# model = CNN(dims=300, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, w2v_path=w2v_path)\n",
    "# train_x, train_y, test_x, test_y, val_x, val_y, n_classes = model.insert_values(train_path,test_path)\n",
    "# his,res,avg = model.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, n=3)\n",
    "# print (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "import random\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self,dims,w2v_path,max_seq_len=20,batch_size=128,epochs=20,chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:            \n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None                \n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None #[tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']        \n",
    "        self.callbacks =None\n",
    "        \n",
    "\n",
    "    def build_lstm(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        lstm_1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_layer)\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(lstm_1)\n",
    "        lstm_2 = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(dropout_out1)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(lstm_2)\n",
    "        dense_1 = layers.Dense(20, activation='relu')(dropout_out2)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dense_1)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        lstm_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        lstm_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #lstm_model.summary()\n",
    "        self.model = lstm_model\n",
    "        \n",
    "    def insert_values(self,train_path,test_path):    \n",
    "        def insert(df):\n",
    "            \n",
    "            # initialize x self.and y matrices\n",
    "            num_lines = len(df)\n",
    "            self.n_classes = df['class'].nunique()          \n",
    "            x_matrix = np.zeros((num_lines, self.max_seq_len ,300))\n",
    "            y_matrix = np.zeros((num_lines, self.n_classes))\n",
    "\n",
    "\n",
    "            # insert values\n",
    "            for i in range(0, num_lines, self.chunk_size):\n",
    "                df_batch = df.iloc[i:i+self.chunk_size]\n",
    "                batch_size = len(df_batch)\n",
    "                x_batch = np.zeros((batch_size, self.max_seq_len, 300))\n",
    "                y_batch = np.zeros((batch_size, self.n_classes))\n",
    "\n",
    "                for j, row in df_batch.iterrows():\n",
    "                    label = row[0]\n",
    "                    sentence = row[1]\n",
    "                    if isinstance(sentence, str):\n",
    "                        words = sentence.split()[:self.max_seq_len]\n",
    "                        for k, word in enumerate(words):\n",
    "                            if word in self.w2v:\n",
    "                                x_batch[j-i, k, :] = self.w2v[word]\n",
    "                    else:\n",
    "                        continue        \n",
    "                    y_batch[j-i,label] = 1.0\n",
    "\n",
    "                x_matrix[i:i+batch_size] = x_batch\n",
    "                y_matrix[i:i+batch_size] = y_batch\n",
    "\n",
    "            return x_matrix,y_matrix\n",
    "    \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()        \n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_x, train_y = insert(train_df)\n",
    "        test_x, test_y = insert(test_df)\n",
    "        val_x, val_y = insert(val_df)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y, val_x, val_y, self.n_classes          \n",
    "\n",
    "    def fit(self,train_x, train_y,  val_x, val_y):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_lstm()  \n",
    "        self.history = self.model.fit(train_x, train_y, batch_size=self.batch_size, epochs=self.epochs, validation_data=(val_x, val_y), callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "    def evaluate(self,test_x, test_y):\n",
    "        return self.model.evaluate(test_x, test_y,return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "    def run_n_times(self,train_x, train_y, test_x, test_y, val_x, val_y, dataset_name, n=3):\n",
    "        \n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_x, train_y, val_x, val_y)\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_lstm()\n",
    "                continue\n",
    "            res = self.evaluate(test_x, test_y)\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "        \n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4)  for metric in res_dict[1].keys()}\n",
    "        \n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_10653/2156677550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhist_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_n_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_10653/2588195815.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims, w2v_path, max_seq_len, batch_size, epochs, chunk_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_path  = 'data/original/cr/train.csv'\n",
    "test_path   = 'data/original/cr/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "name = 'cr'\n",
    "max_seq_len = 64\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "lstm = LSTM(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs)\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, n_classes = lstm.insert_values(train_path,test_path)\n",
    "hist_dict, res_dict, avg_dict = lstm.run_n_times(train_x, train_y, test_x, test_y, val_x, val_y, name, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, dims, w2v_path, max_seq_len=20, batch_size=128, epochs=20, chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:\n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None\n",
    "        self.callbacks = None\n",
    "\n",
    "    def build_cnn(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        conv1_1 = layers.Conv1D(128, 4, activation='relu', padding='same')(input_layer)\n",
    "        conv1_2 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_1)\n",
    "        #conv1_3 = layers.Conv1D(128, 5, activation='relu', padding='same')(conv1_2)\n",
    "        conv_out = layers.Concatenate(axis=1)([conv1_1, conv1_2])\n",
    "\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(conv_out)\n",
    "\n",
    "        pool_out = layers.MaxPool1D(pool_size=self.max_seq_len, padding='valid')(dropout_out1)\n",
    "        flatten_out = layers.Flatten()(pool_out)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(flatten_out)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dropout_out2)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        cnn_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        cnn_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #cnn_model.summary()\n",
    "        self.model = cnn_model\n",
    "\n",
    "    def prepare_dataset(self, df):\n",
    "        def generator():\n",
    "            for _, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                x = np.zeros((self.max_seq_len, 300))\n",
    "                y = np.zeros(self.n_classes)\n",
    "\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for k, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x[k, :] = self.w2v[word]\n",
    "                y[label] = 1.0\n",
    "                yield x, y\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(self.max_seq_len, 300), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.n_classes,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def insert_values(self, train_path, test_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()\n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_dataset = self.prepare_dataset(train_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_dataset = self.prepare_dataset(test_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_dataset = self.prepare_dataset(val_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, test_dataset, val_dataset, self.n_classes\n",
    "\n",
    "    def fit(self, train_dataset, val_dataset):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_cnn()\n",
    "        self.history = self.model.fit(train_dataset, epochs=self.epochs, validation_data=val_dataset, callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        return self.model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "    def run_n_times(self, train_dataset, test_dataset, val_dataset, dataset_name, n=3):\n",
    "\n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_dataset, val_dataset)  # Updated to use train_dataset and val_dataset\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_cnn()\n",
    "                continue\n",
    "            res = self.evaluate(test_dataset)  # Updated to use test_dataset\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "\n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "\n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        with open(f\"results/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return hist_dict, res_dict, avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cardio'\n",
    "train_path = f'data/original/{name}/train.csv'\n",
    "test_path = f'data/original/{name}/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "dataset_name = f'{name}'\n",
    "max_seq_len = 64\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "cnn = CNN(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, chunk_size=1000)\n",
    "train_dataset, test_dataset, val_dataset, n_classes = cnn.insert_values(train_path, test_path)  # Updated to return datasets\n",
    "hist_dict, res_dict, avg_dict = cnn.run_n_times(train_dataset, test_dataset, val_dataset, name, n=3)  # Updated to use datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# = = = = =  = = = = = = =\n",
    "# L  S  T  M\n",
    "# = = = = = = = = = = = = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, dims, w2v_path, max_seq_len=20, batch_size=128, epochs=20, chunk_size=1000):\n",
    "        self.dims = dims\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.epochs = epochs\n",
    "        with open(w2v_path, 'rb') as f:\n",
    "            self.w2v = pickle.load(f)\n",
    "        self.model = None\n",
    "        self.n_classes = None\n",
    "        self.history = None\n",
    "        self.metrics = None\n",
    "        self.callbacks = None\n",
    "\n",
    "    def build_lstm(self):\n",
    "        if self.n_classes > 2:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            activation = 'softmax'\n",
    "        else:\n",
    "            loss = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len, 300))\n",
    "        lstm_1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_layer)\n",
    "        dropout_rate = 0.5\n",
    "        dropout_out1 = layers.Dropout(dropout_rate)(lstm_1)\n",
    "        lstm_2 = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(dropout_out1)\n",
    "        dropout_out2 = layers.Dropout(dropout_rate)(lstm_2)\n",
    "        dense_1 = layers.Dense(20, activation='relu')(dropout_out2)\n",
    "        dense_out = layers.Dense(self.n_classes, activation=activation, kernel_regularizer=regularizers.L2(0.001))(dense_1)\n",
    "        \n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        lstm_model = Model(inputs=input_layer, outputs=dense_out)\n",
    "        lstm_model.compile(optimizer='adam', loss=loss, metrics=self.metrics)\n",
    "        #lstm_model.summary()\n",
    "        self.model = lstm_model\n",
    "\n",
    "    def prepare_dataset(self, df):\n",
    "        def generator():\n",
    "            for _, row in df.iterrows():\n",
    "                label = row[0]\n",
    "                sentence = row[1]\n",
    "                x = np.zeros((self.max_seq_len, 300))\n",
    "                y = np.zeros(self.n_classes)\n",
    "\n",
    "                if isinstance(sentence, str):\n",
    "                    words = sentence.split()[:self.max_seq_len]\n",
    "                    for k, word in enumerate(words):\n",
    "                        if word in self.w2v:\n",
    "                            x[k, :] = self.w2v[word]\n",
    "                y[label] = 1.0\n",
    "                yield x, y\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(self.max_seq_len, 300), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.n_classes,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def insert_values(self, train_path, test_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        self.n_classes = train_df['class'].nunique()\n",
    "        unique_classes = train_df['class'].unique()\n",
    "        labels_map = dict(zip(unique_classes, range(self.n_classes)))\n",
    "\n",
    "        train_df['class'] = train_df['class'].map(labels_map)\n",
    "        test_df['class'] = test_df['class'].map(labels_map)\n",
    "\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=100)\n",
    "        print(f'Train size: {len(train_df)}\\nValidation size: {len(val_df)}\\nTest size: {len(test_df)}')\n",
    "\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "        val_df = val_df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
    "\n",
    "        train_dataset = self.prepare_dataset(train_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_dataset = self.prepare_dataset(test_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_dataset = self.prepare_dataset(val_df).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_dataset, test_dataset, val_dataset, self.n_classes\n",
    "\n",
    "    def fit(self, train_dataset, val_dataset):\n",
    "        self.metrics = [tf.keras.metrics.AUC(name='auc'), tfa.metrics.F1Score(self.n_classes, average='weighted', name='f1_score'), 'accuracy']\n",
    "        self.build_lstm()\n",
    "        self.history = self.model.fit(train_dataset, epochs=self.epochs, validation_data=val_dataset, callbacks=self.callbacks, verbose=0)\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        return self.model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "    def run_n_times(self, train_dataset, test_dataset, val_dataset, dataset_name, n=3):\n",
    "\n",
    "        log_dir = f\"logs/fit/{dataset_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        decay_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001 ,min_lr=0.00001)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        self.callbacks = [tensorboard_callback, decay_rate, early_stopping]\n",
    "\n",
    "        hist_dict = {}\n",
    "        res_dict = {}\n",
    "        best_val_loss = float('inf')\n",
    "        for i in range(n):\n",
    "            print(f'Run {i+1} of {n}')\n",
    "            try:\n",
    "                self.fit(train_dataset, val_dataset)  # Updated to use train_dataset and val_dataset\n",
    "            except tf.errors.ResourceExhaustedError:\n",
    "                K.clear_session()\n",
    "                self.model = None\n",
    "                self.build_lstm()\n",
    "                continue\n",
    "            res = self.evaluate(test_dataset)  # Updated to use test_dataset\n",
    "            res_dict[i+1] = res\n",
    "            if self.history.history['val_loss'][-1] < best_val_loss:\n",
    "                best_val_loss = self.history.history['val_loss'][-1]\n",
    "                self.model.save(f\"models/lstm/{dataset_name}_best_model.h5\")\n",
    "            self.model.set_weights([np.zeros(w.shape) for w in self.model.get_weights()])\n",
    "\n",
    "        avg_dict = {metric: round(sum(values[metric] for values in res_dict.values()) / len(res_dict), 4) for metric in res_dict[1].keys()}\n",
    "\n",
    "        # Save the average results to disk\n",
    "        os.makedirs(\"results/lstm\", exist_ok=True)\n",
    "        with open(f\"results/lstm/{dataset_name}_avg_results.txt\", \"w\") as f:\n",
    "            for key, value in avg_dict.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        return hist_dict, res_dict, avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'cr'\n",
    "train_path = f'data/original/{name}/train.csv'\n",
    "test_path = f'data/original/{name}/test.csv'\n",
    "w2v_path = 'w2v.pkl'\n",
    "dataset_name = f'{name}'\n",
    "max_seq_len = 128\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "lstm = LSTM(dims=300, w2v_path=w2v_path, max_seq_len=max_seq_len, batch_size=batch_size, epochs=epochs, chunk_size=1000)\n",
    "train_dataset, test_dataset, val_dataset, n_classes = lstm.insert_values(train_path, test_path)  # Updated to return datasets\n",
    "hist_dict, res_dict, avg_dict = lstm.run_n_times(train_dataset, test_dataset, val_dataset, name, n=3)  # Updated to use datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B   E   R   T\n",
    "# ==========================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=<number_of_classes>)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRAIN-ENV",
   "language": "python",
   "name": "train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
