{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_glove_embeddings(path):\n",
    "    word2vec_dict = {}            \n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                word2vec_dict[word] = vector\n",
    "            except ValueError:                \n",
    "                continue    \n",
    "    return word2vec_dict\n",
    "\n",
    "path = \"glove.840B.300d.txt\"\n",
    "word2vec = load_glove_embeddings(path)\n",
    "print(len(word2vec))\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(word2vec, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- process the fraction of datasets ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "namelist = ['cr', 'trec', 'agnews', 'pc', 'yelp', 'kaggle_med', 'cardio', 'bbc', 'sst2','subj']\n",
    "for name in namelist:\n",
    "    df = pd.read_csv(f'data/original/{name}/train.csv')\n",
    "    df = df.sample(frac=0.5, random_state=100)\n",
    "    df.to_csv(f'data/original/{name}/train_50.csv', index=False)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- preprocess the data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and remove all rows with nan or empty string or space or NONE or None or none or NaN or nan or NaT or nat or N/A or n/a or NULL or null or Null or nil or NIL or Nil or na or NA or n.a. or N.A. or n.a or N.a or N.A or n.A or n.A. or N.a. or N.A. or n.A. or n.A. or N.A. or n.a. or N.a\n",
    "# and print rows that removed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    invalid_values = ['', ' ', 'NONE', 'None', 'none', 'NaN', 'nan', 'NaT', 'nat', 'N/A', 'n/a',\n",
    "                        'NULL', 'null', 'Null', 'nil', 'NIL', 'Nil', 'na', 'NA', 'n.a.', 'N.A.', 'n.a', 'N.a', 'N.A',\n",
    "                          'n.A', 'n.A.', 'N.a.', 'N.A.', 'n.A.', 'n.A.', 'N.A.', 'n.a.', 'N.a']\n",
    "    for text in df['text']:\n",
    "        if text in invalid_values and len(text) < 4:\n",
    "            # delete the row\n",
    "\n",
    "            # reset index and save to csv\n",
    "\n",
    "            \n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namelist = ['cr', 'trec', 'agnews', 'pc', 'yelp', 'kaggle_med', 'cardio', 'bbc', 'sst2','subj']\n",
    "for name in namelist:\n",
    "    clean(f'data/original/{name}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    invalid_values = ['', ' ', 'NONE', 'None', 'none', 'NaN', 'nan', 'NaT', 'nat', 'N/A', 'n/a',\n",
    "                        'NULL', 'null', 'Null', 'nil', 'NIL', 'Nil', 'na', 'NA', 'n.a.', 'N.A.', 'n.a', 'N.a', 'N.A',\n",
    "                          'n.A', 'n.A.', 'N.a.', 'N.A.', 'n.A.', 'n.A.', 'N.A.', 'n.a.', 'N.a']\n",
    "    \n",
    "    invalid_rows = df[df['text'].apply(lambda x: x in invalid_values and len(x) < 4) | df['class'].apply(lambda x: x in invalid_values and len(x) < 4)]\n",
    "\n",
    "    # Print the removed rows\n",
    "    print(\"Removed rows:\")\n",
    "    print(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_path = 'data/original/pubmed/train.csv'\n",
    "test_path = 'data/original/pubmed/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "train_df['class'] = train_df['class'].map({'OBJECTIVE': 0, 'METHODS': 1, 'RESULTS': 2, 'CONCLUSIONS': 3, 'BACKGROUND': 4})\n",
    "test_df['class'] = test_df['class'].map({'OBJECTIVE': 0, 'METHODS': 1, 'RESULTS': 2, 'CONCLUSIONS': 3, 'BACKGROUND': 4})\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i in ['10', '20', '50']:\n",
    "    train_path = f'data/original/pubmed/train_{i}.csv'\n",
    "    df = pd.read_csv(train_path)\n",
    "    df['class'] = df['class'].map({'OBJECTIVE': 0, 'METHODS': 1, 'RESULTS': 2, 'CONCLUSIONS': 3, 'BACKGROUND': 4})\n",
    "    df.to_csv(train_path, index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploring the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_results_into_df(path, model_list, dataset_list, percentage_list, numbers):\n",
    "    results = {}\n",
    "\n",
    "    for number, percentage in zip(numbers, percentage_list):\n",
    "        for model in model_list:\n",
    "            for dataset in dataset_list:\n",
    "                try:\n",
    "                    results[f'{model}_{dataset}_{percentage}'] = []\n",
    "                    with open(f'{path}/{model}/{percentage}/{dataset}_{number}_results.txt', 'r') as f:\n",
    "                        for line in f:\n",
    "                            results[f'{model}_{dataset}_{percentage}'].append(line.strip())    \n",
    "                except Exception as e:\n",
    "                    if dataset != 'kaggle_med':\n",
    "                        print(f\"Error occurred: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "    df['f1_score'] = None\n",
    "    df['accuracy'] = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for item in row:\n",
    "            if item is not None:\n",
    "                metric, value = item.split(\":\")\n",
    "                if metric.strip() in ['f1', 'f1_score']:\n",
    "                    df.at[index, 'f1_score'] = float(value)\n",
    "                elif metric.strip() in ['acc', 'accuracy']:\n",
    "                    df.at[index, 'accuracy'] = float(value)\n",
    "\n",
    "    df = df[['f1_score', 'accuracy']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploring the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "new_dict = {}\n",
    "path = 'results/original'\n",
    "model_list = ['bert', 'lstm','cnn'] # 3 models\n",
    "dataset_list = ['cr', 'trec', 'agnews', 'pc', 'yelp', 'cardio', 'bbc', 'sst2','subj', 'pubmed'] # 10 datasets\n",
    "percentage_list = ['10_percent', '20_percent', '50_percent','full'] # 4 percentages\n",
    "numbers = ['10', '20', '50', 'full']\n",
    "\n",
    "for number, percentage in zip(numbers, percentage_list):\n",
    "    for model in model_list:\n",
    "        for dataset in dataset_list:\n",
    "            try:\n",
    "                new_dict[f'{model}_{dataset}_{percentage}'] = []\n",
    "                with open(f'{path}/{model}/{percentage}/{dataset}_{number}_results.txt', 'r') as f:\n",
    "                    for line in f:\n",
    "                        new_dict[f'{model}_{dataset}_{percentage}'].append(line.strip())    \n",
    "            except Exception as e:\n",
    "                if dataset != 'kaggle_med':\n",
    "                    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(new_dict, orient='index')\n",
    "# create empty columns\n",
    "df['f1_score'] = None\n",
    "df['accuracy'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert_cr_10_percent</th>\n",
       "      <td>acc: 0.8511</td>\n",
       "      <td>f1: 0.8866</td>\n",
       "      <td>prec: 0.8833</td>\n",
       "      <td>rec: 0.8909</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_trec_10_percent</th>\n",
       "      <td>acc: 0.668</td>\n",
       "      <td>f1: 0.557</td>\n",
       "      <td>prec: 0.5605</td>\n",
       "      <td>rec: 0.6375</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_agnews_10_percent</th>\n",
       "      <td>acc: 0.8883</td>\n",
       "      <td>f1: 0.8882</td>\n",
       "      <td>prec: 0.8883</td>\n",
       "      <td>rec: 0.8883</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_pc_10_percent</th>\n",
       "      <td>acc: 0.8824</td>\n",
       "      <td>f1: 0.8796</td>\n",
       "      <td>prec: 0.8639</td>\n",
       "      <td>rec: 0.8959</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_yelp_10_percent</th>\n",
       "      <td>acc: 0.446</td>\n",
       "      <td>f1: 0.2984</td>\n",
       "      <td>prec: 0.3118</td>\n",
       "      <td>rec: 0.3616</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_cardio_10_percent</th>\n",
       "      <td>acc: 0.3682</td>\n",
       "      <td>f1: 0.309</td>\n",
       "      <td>prec: 0.2966</td>\n",
       "      <td>rec: 0.3702</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_bbc_10_percent</th>\n",
       "      <td>acc: 0.6338</td>\n",
       "      <td>f1: 0.5657</td>\n",
       "      <td>prec: 0.6224</td>\n",
       "      <td>rec: 0.7381</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_sst2_10_percent</th>\n",
       "      <td>acc: 0.8669</td>\n",
       "      <td>f1: 0.8627</td>\n",
       "      <td>prec: 0.8376</td>\n",
       "      <td>rec: 0.8894</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_subj_10_percent</th>\n",
       "      <td>acc: 0.94</td>\n",
       "      <td>f1: 0.9386</td>\n",
       "      <td>prec: 0.9316</td>\n",
       "      <td>rec: 0.946</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_pubmed_10_percent</th>\n",
       "      <td>acc: 0.841</td>\n",
       "      <td>f1: 0.7812</td>\n",
       "      <td>prec: 0.7762</td>\n",
       "      <td>rec: 0.7922</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_cr_10_percent</th>\n",
       "      <td>loss: 0.5325</td>\n",
       "      <td>auc: 0.8222</td>\n",
       "      <td>f1_score: 0.7437</td>\n",
       "      <td>accuracy: 0.7422</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_trec_10_percent</th>\n",
       "      <td>loss: 0.8045</td>\n",
       "      <td>auc: 0.936</td>\n",
       "      <td>f1_score: 0.737</td>\n",
       "      <td>accuracy: 0.7447</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_agnews_10_percent</th>\n",
       "      <td>loss: 0.3503</td>\n",
       "      <td>auc: 0.9777</td>\n",
       "      <td>f1_score: 0.8836</td>\n",
       "      <td>accuracy: 0.884</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_pc_10_percent</th>\n",
       "      <td>loss: 0.277</td>\n",
       "      <td>auc: 0.9605</td>\n",
       "      <td>f1_score: 0.891</td>\n",
       "      <td>accuracy: 0.8912</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_yelp_10_percent</th>\n",
       "      <td>loss: 1.3659</td>\n",
       "      <td>auc: 0.7583</td>\n",
       "      <td>f1_score: 0.3568</td>\n",
       "      <td>accuracy: 0.3907</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_cardio_10_percent</th>\n",
       "      <td>loss: 2.264</td>\n",
       "      <td>auc: 0.8494</td>\n",
       "      <td>f1_score: 0.3097</td>\n",
       "      <td>accuracy: 0.3321</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_bbc_10_percent</th>\n",
       "      <td>loss: 0.277</td>\n",
       "      <td>auc: 0.9856</td>\n",
       "      <td>f1_score: 0.9323</td>\n",
       "      <td>accuracy: 0.9327</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_sst2_10_percent</th>\n",
       "      <td>loss: 0.4941</td>\n",
       "      <td>auc: 0.8559</td>\n",
       "      <td>f1_score: 0.7831</td>\n",
       "      <td>accuracy: 0.7838</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_subj_10_percent</th>\n",
       "      <td>loss: 0.3352</td>\n",
       "      <td>auc: 0.9503</td>\n",
       "      <td>f1_score: 0.894</td>\n",
       "      <td>accuracy: 0.894</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm_pubmed_10_percent</th>\n",
       "      <td>loss: 0.5921</td>\n",
       "      <td>auc: 0.9575</td>\n",
       "      <td>f1_score: 0.7913</td>\n",
       "      <td>accuracy: 0.7941</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0            1                 2  \\\n",
       "bert_cr_10_percent       acc: 0.8511   f1: 0.8866      prec: 0.8833   \n",
       "bert_trec_10_percent      acc: 0.668    f1: 0.557      prec: 0.5605   \n",
       "bert_agnews_10_percent   acc: 0.8883   f1: 0.8882      prec: 0.8883   \n",
       "bert_pc_10_percent       acc: 0.8824   f1: 0.8796      prec: 0.8639   \n",
       "bert_yelp_10_percent      acc: 0.446   f1: 0.2984      prec: 0.3118   \n",
       "bert_cardio_10_percent   acc: 0.3682    f1: 0.309      prec: 0.2966   \n",
       "bert_bbc_10_percent      acc: 0.6338   f1: 0.5657      prec: 0.6224   \n",
       "bert_sst2_10_percent     acc: 0.8669   f1: 0.8627      prec: 0.8376   \n",
       "bert_subj_10_percent       acc: 0.94   f1: 0.9386      prec: 0.9316   \n",
       "bert_pubmed_10_percent    acc: 0.841   f1: 0.7812      prec: 0.7762   \n",
       "lstm_cr_10_percent      loss: 0.5325  auc: 0.8222  f1_score: 0.7437   \n",
       "lstm_trec_10_percent    loss: 0.8045   auc: 0.936   f1_score: 0.737   \n",
       "lstm_agnews_10_percent  loss: 0.3503  auc: 0.9777  f1_score: 0.8836   \n",
       "lstm_pc_10_percent       loss: 0.277  auc: 0.9605   f1_score: 0.891   \n",
       "lstm_yelp_10_percent    loss: 1.3659  auc: 0.7583  f1_score: 0.3568   \n",
       "lstm_cardio_10_percent   loss: 2.264  auc: 0.8494  f1_score: 0.3097   \n",
       "lstm_bbc_10_percent      loss: 0.277  auc: 0.9856  f1_score: 0.9323   \n",
       "lstm_sst2_10_percent    loss: 0.4941  auc: 0.8559  f1_score: 0.7831   \n",
       "lstm_subj_10_percent    loss: 0.3352  auc: 0.9503   f1_score: 0.894   \n",
       "lstm_pubmed_10_percent  loss: 0.5921  auc: 0.9575  f1_score: 0.7913   \n",
       "\n",
       "                                       3 f1_score accuracy  \n",
       "bert_cr_10_percent           rec: 0.8909     None     None  \n",
       "bert_trec_10_percent         rec: 0.6375     None     None  \n",
       "bert_agnews_10_percent       rec: 0.8883     None     None  \n",
       "bert_pc_10_percent           rec: 0.8959     None     None  \n",
       "bert_yelp_10_percent         rec: 0.3616     None     None  \n",
       "bert_cardio_10_percent       rec: 0.3702     None     None  \n",
       "bert_bbc_10_percent          rec: 0.7381     None     None  \n",
       "bert_sst2_10_percent         rec: 0.8894     None     None  \n",
       "bert_subj_10_percent          rec: 0.946     None     None  \n",
       "bert_pubmed_10_percent       rec: 0.7922     None     None  \n",
       "lstm_cr_10_percent      accuracy: 0.7422     None     None  \n",
       "lstm_trec_10_percent    accuracy: 0.7447     None     None  \n",
       "lstm_agnews_10_percent   accuracy: 0.884     None     None  \n",
       "lstm_pc_10_percent      accuracy: 0.8912     None     None  \n",
       "lstm_yelp_10_percent    accuracy: 0.3907     None     None  \n",
       "lstm_cardio_10_percent  accuracy: 0.3321     None     None  \n",
       "lstm_bbc_10_percent     accuracy: 0.9327     None     None  \n",
       "lstm_sst2_10_percent    accuracy: 0.7838     None     None  \n",
       "lstm_subj_10_percent     accuracy: 0.894     None     None  \n",
       "lstm_pubmed_10_percent  accuracy: 0.7941     None     None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    for item in row:\n",
    "        if item is not None:\n",
    "            metric, value = item.split(\":\")\n",
    "            if metric.strip() in ['f1', 'f1_score']:\n",
    "                df.at[index, 'f1_score'] = float(value)\n",
    "            elif metric.strip() in ['acc', 'accuracy']:\n",
    "                df.at[index, 'accuracy'] = float(value)\n",
    "\n",
    "# keep only relevant columns\n",
    "df = df[['f1_score', 'accuracy']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating datasets from 4 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "aug_path = 'data/augmented/{dataset_name}/meth_{method_name}_pctwts_0.5_example_4.csv'\n",
    "org_path = 'data/original/agnews/train.csv'\n",
    "df_aug = pd.read_csv(aug_path)\n",
    "df_org = pd.read_csv(org_path)\n",
    "\n",
    "df_aug[\"org\"] = df_aug[\"text\"].isin(df_org[\"text\"])\n",
    "df_aug['first_aug'] = df_aug['text'].shift(-1)\n",
    "df_aug['first_aug'] = df_aug['first_aug'].where(df_aug['first_aug'] != df_aug['text'], None)\n",
    "df_aug['second_aug'] = df_aug['text'].shift(-2)\n",
    "df_aug['second_aug'] = df_aug['second_aug'].where(df_aug['second_aug'] != df_aug['text'], None)\n",
    "na_indices = df_aug[df_aug['first_aug'].isna() | df_aug['second_aug'].isna()].index\n",
    "df_aug = df_aug.drop(na_indices)\n",
    "\n",
    "# Selecting rows where 'org' is True and only keeping 'class', 'text', 'first_aug', and 'second_aug' columns\n",
    "df_result = df_aug[df_aug['org'] == True][['class', 'text', 'first_aug', 'second_aug']]\n",
    "\n",
    "# Remove rows where 'second_aug' is in the original text\n",
    "# Remove rows where 'first_aug' is in the original text\n",
    "\n",
    "df_result = df_result[~df_result['first_aug'].isin(df_aug[df_aug['org'] == True]['text'])]\n",
    "df_result = df_result[~df_result['second_aug'].isin(df_aug[df_aug['org'] == True]['text'])]\n",
    "\n",
    "df_first_aug = df_result[['class', 'first_aug']].copy()\n",
    "df_first_aug.rename(columns={'first_aug': 'text'}, inplace=True)\n",
    "\n",
    "df_second_aug = df_result[['class', 'second_aug']].copy()\n",
    "df_second_aug.rename(columns={'second_aug': 'text'}, inplace=True)\n",
    "\n",
    "df_result['aug_number'] = 'original'\n",
    "df_first_aug['aug_number'] = 'first_aug'\n",
    "df_second_aug['aug_number'] = 'second_aug'\n",
    "\n",
    "df_all = pd.concat([df_result, df_first_aug, df_second_aug])\n",
    "\n",
    "df_all.sort_index(inplace=True)\n",
    "\n",
    "df_one_example = df_all[df_all['aug_number'].isin(['original', 'first_aug'])]\n",
    "df_two_examples = df_all[df_all['aug_number'].isin(['original', 'first_aug', 'second_aug'])]\n",
    "\n",
    "df_one_example = df_one_example[['class', 'text']]\n",
    "df_two_examples = df_two_examples[['class', 'text']]\n",
    "\n",
    "df_one_example.to_csv('data/augmented/agnews/meth_eda_pctwts_0.5_example_1.csv', index=False)\n",
    "df_two_examples.to_csv('data/augmented/agnews/meth_eda_pctwts_0.5_example_2.csv', index=False)\n",
    "\n",
    "\n",
    "def create_aug_df_from_4_example(dataset_name,method_name):\n",
    "    aug_path = f'data/augmented/{dataset_name}/meth_{method_name}_pctwts_0.5_example_4.csv'\n",
    "    org_path = f'data/original/{dataset_name}/train.csv'\n",
    "    df_aug = pd.read_csv(aug_path)\n",
    "    df_org = pd.read_csv(org_path)\n",
    "    df_aug[\"org\"] = df_aug[\"text\"].isin(df_org[\"text\"])\n",
    "    df_aug['first_aug'] = df_aug['text'].shift(-1)\n",
    "    df_aug['first_aug'] = df_aug['first_aug'].where(df_aug['first_aug'] != df_aug['text'], None)\n",
    "    df_aug['second_aug'] = df_aug['text'].shift(-2)\n",
    "    df_aug['second_aug'] = df_aug['second_aug'].where(df_aug['second_aug'] != df_aug['text'], None)\n",
    "    na_indices = df_aug[df_aug['first_aug'].isna() | df_aug['second_aug'].isna()].index\n",
    "    df_aug = df_aug.drop(na_indices)\n",
    "\n",
    "    # Selecting rows where 'org' is True and only keeping 'class', 'text', 'first_aug', and 'second_aug' columns\n",
    "    df_result = df_aug[df_aug['org'] == True][['class', 'text', 'first_aug', 'second_aug']]\n",
    "\n",
    "    # Remove rows where 'second_aug' is in the original text\n",
    "    # Remove rows where 'first_aug' is in the original text\n",
    "\n",
    "    df_result = df_result[~df_result['first_aug'].isin(df_aug[df_aug['org'] == True]['text'])]\n",
    "    df_result = df_result[~df_result['second_aug'].isin(df_aug[df_aug['org'] == True]['text'])]\n",
    "\n",
    "    df_first_aug = df_result[['class', 'first_aug']].copy()\n",
    "    df_first_aug.rename(columns={'first_aug': 'text'}, inplace=True)\n",
    "\n",
    "    df_second_aug = df_result[['class', 'second_aug']].copy()\n",
    "    df_second_aug.rename(columns={'second_aug': 'text'}, inplace=True)\n",
    "\n",
    "    df_result['aug_number'] = 'original'\n",
    "    df_first_aug['aug_number'] = 'first_aug'\n",
    "    df_second_aug['aug_number'] = 'second_aug'\n",
    "\n",
    "    df_all = pd.concat([df_result, df_first_aug, df_second_aug])\n",
    "\n",
    "    df_all.sort_index(inplace=True)\n",
    "\n",
    "    df_one_example = df_all[df_all['aug_number'].isin(['original', 'first_aug'])]\n",
    "    df_two_examples = df_all[df_all['aug_number'].isin(['original', 'first_aug', 'second_aug'])]\n",
    "\n",
    "    df_one_example = df_one_example[['class', 'text']]\n",
    "    df_two_examples = df_two_examples[['class', 'text']]\n",
    "\n",
    "    df_one_example.to_csv('data/augmented/{dataset_name}/meth_{method_name}_pctwts_0.5_example_1.csv', index=False)\n",
    "    df_two_examples.to_csv('data/augmented/{dataset_name}/meth_{method_name}_pctwts_0.5_example_2.csv', index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def remove_na_augmentations(df_aug, df_org):\n",
    "    \"\"\"\n",
    "    This function identifies original and augmented texts and removes NA augmentations.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_aug: DataFrame, the augmented data\n",
    "    - df_org: DataFrame, the original data\n",
    "\n",
    "    Returns:\n",
    "    - df_aug: DataFrame, the augmented data with NAs removed\n",
    "    \"\"\"\n",
    "    df_aug[\"org\"] = df_aug[\"text\"].isin(df_org[\"text\"])\n",
    "    df_aug['first_aug'] = df_aug['text'].shift(-1)\n",
    "    df_aug['first_aug'] = df_aug['first_aug'].where(df_aug['first_aug'] != df_aug['text'], None)\n",
    "    df_aug['second_aug'] = df_aug['text'].shift(-2)\n",
    "    df_aug['second_aug'] = df_aug['second_aug'].where(df_aug['second_aug'] != df_aug['text'], None)\n",
    "    na_indices = df_aug[df_aug['first_aug'].isna() | df_aug['second_aug'].isna()].index\n",
    "    df_aug = df_aug.drop(na_indices)\n",
    "\n",
    "    return df_aug\n",
    "\n",
    "\n",
    "def select_org_and_aug_cols(df_aug):\n",
    "    \"\"\"\n",
    "    This function selects relevant columns of original and augmented texts.\n",
    "\n",
    "    Parameters:\n",
    "    - df_aug: DataFrame, the augmented data\n",
    "\n",
    "    Returns:\n",
    "    - df_result: DataFrame, the selected columns from the data\n",
    "    \"\"\"\n",
    "    df_result = df_aug[df_aug['org'] == True][['class', 'text', 'first_aug', 'second_aug']]\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def create_augmentations(df_result, df_aug):\n",
    "    \"\"\"\n",
    "    This function creates DataFrame of original and augmented texts.\n",
    "\n",
    "    Parameters:\n",
    "    - df_result: DataFrame, the selected columns from the data\n",
    "    - df_aug: DataFrame, the augmented data\n",
    "\n",
    "    Returns:\n",
    "    - df_one_example: DataFrame, one example of augmented data\n",
    "    - df_two_examples: DataFrame, two examples of augmented data\n",
    "    \"\"\"\n",
    "    df_result = df_result[~df_result['first_aug'].isin(df_aug[df_aug['org'] == True]['text'])]\n",
    "    df_result = df_result[~df_result['second_aug'].isin(df_aug[df_aug['org'] == True]['text'])]\n",
    "\n",
    "    df_first_aug = df_result[['class', 'first_aug']].copy()\n",
    "    df_first_aug.rename(columns={'first_aug': 'text'}, inplace=True)\n",
    "\n",
    "    df_second_aug = df_result[['class', 'second_aug']].copy()\n",
    "    df_second_aug.rename(columns={'second_aug': 'text'}, inplace=True)\n",
    "\n",
    "    df_result['aug_number'] = 'original'\n",
    "    df_first_aug['aug_number'] = 'first_aug'\n",
    "    df_second_aug['aug_number'] = 'second_aug'\n",
    "\n",
    "    df_all = pd.concat([df_result, df_first_aug, df_second_aug])\n",
    "\n",
    "    df_all.sort_index(inplace=True)\n",
    "\n",
    "    df_one_example = df_all[df_all['aug_number'].isin(['original', 'first_aug'])]\n",
    "    df_two_examples = df_all[df_all['aug_number'].isin(['original', 'first_aug', 'second_aug'])]\n",
    "\n",
    "    df_one_example = df_one_example[['class', 'text']]\n",
    "    df_two_examples = df_two_examples[['class', 'text']]\n",
    "\n",
    "    return df_one_example, df_two_examples\n",
    "\n",
    "\n",
    "def create_aug_df_from_4_example(dataset_name,method_name):\n",
    "    \"\"\"\n",
    "    This function creates a dataframe with one and two augmented versions from the original \n",
    "    and four augmentations.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_name: str, name of the dataset\n",
    "    - method_name: str, name of the method used for data augmentation\n",
    "\n",
    "    Returns:\n",
    "    - None. The function writes the output to CSV files.\n",
    "    \"\"\"\n",
    "    aug_path = f'data/augmented/{dataset_name}/meth_{method_name}_pctwts_0.5_example_4.csv'\n",
    "    org_path = f'data/original/{dataset_name}/train.csv'\n",
    "    \n",
    "    df_aug = pd.read_csv(aug_path)\n",
    "    df_org = pd.read_csv(org_path)\n",
    "    \n",
    "    df_aug = remove_na_augmentations(df_aug, df_org)\n",
    "    df_result = select_org_and_aug_cols(df_aug)\n",
    "    \n",
    "    df_one_example, df_two_examples = create_augmentations(df_result, df_aug)\n",
    "    \n",
    "    df_one_example.to_csv(f'data/augmented/{dataset_name}/meth_{method_name}_pctwts_0.5_example_1.csv', index=False)\n",
    "    df_two_examples.to_csv(f'data/augmented/{dataset_name}/meth_{method_name}_pctwts_0.5_example_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_aug_df_from_4_example('yelp','checklist')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'results/original'\n",
    "model_list = ['bert', 'lstm','cnn']\n",
    "dataset_list = ['cr', 'trec', 'agnews', 'pc', 'yelp', 'cardio', 'bbc', 'sst2','subj', 'pubmed']\n",
    "percentage_list = ['10_percent', '20_percent', '50_percent','full']\n",
    "numbers = ['10', '20', '50', 'full']\n",
    "\n",
    "df = load_results_into_df(path, model_list, dataset_list, percentage_list, numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert_cr_10_percent</th>\n",
       "      <td>0.8866</td>\n",
       "      <td>0.8511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_trec_10_percent</th>\n",
       "      <td>0.557</td>\n",
       "      <td>0.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_agnews_10_percent</th>\n",
       "      <td>0.8882</td>\n",
       "      <td>0.8883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_pc_10_percent</th>\n",
       "      <td>0.8796</td>\n",
       "      <td>0.8824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_yelp_10_percent</th>\n",
       "      <td>0.2984</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_cardio_full</th>\n",
       "      <td>0.2882</td>\n",
       "      <td>0.3083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_bbc_full</th>\n",
       "      <td>0.9522</td>\n",
       "      <td>0.9522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_sst2_full</th>\n",
       "      <td>0.8365</td>\n",
       "      <td>0.8367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_subj_full</th>\n",
       "      <td>0.9066</td>\n",
       "      <td>0.9067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_pubmed_full</th>\n",
       "      <td>0.813</td>\n",
       "      <td>0.8192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       f1_score accuracy\n",
       "bert_cr_10_percent       0.8866   0.8511\n",
       "bert_trec_10_percent      0.557    0.668\n",
       "bert_agnews_10_percent   0.8882   0.8883\n",
       "bert_pc_10_percent       0.8796   0.8824\n",
       "bert_yelp_10_percent     0.2984    0.446\n",
       "...                         ...      ...\n",
       "cnn_cardio_full          0.2882   0.3083\n",
       "cnn_bbc_full             0.9522   0.9522\n",
       "cnn_sst2_full            0.8365   0.8367\n",
       "cnn_subj_full            0.9066   0.9067\n",
       "cnn_pubmed_full           0.813   0.8192\n",
       "\n",
       "[120 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
