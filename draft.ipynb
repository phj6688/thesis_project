{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/agnews/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 titles and descriptions in the dataset full text\n",
    "for i in range(3):\n",
    "    title = df['Title'][i]\n",
    "    description = df['Description'][i]\n",
    "    label = df['Class Index'][i]\n",
    "    num_tokens = description.count(' ') + 1\n",
    "    print('Title: ', title)\n",
    "    print('Description: ', description)\n",
    "    print('Number of tokens: ', num_tokens)\n",
    "    print('Label: ', label)\n",
    "\n",
    "    print('===========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(df)):    \n",
    "    description = df['Description'][i]    \n",
    "    num_tokens = description.count(' ') + 1\n",
    "    tokens.append(num_tokens)\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[['Class Index', 'Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/agnews/train.csv')\n",
    "test_df = pd.read_csv('./data/agnews/test.csv')\n",
    "\n",
    "train_df = train_df[['Class Index', 'Description']]\n",
    "test_df = test_df[['Class Index', 'Description']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savetxt('./data/agnews/train.txt', train_df.values, fmt='%s', delimiter='\\t')\n",
    "np.savetxt('./data/agnews/test.txt', test_df.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med = pd.read_csv('./data/original/kaggle_med/train.txt',delimiter='\\t', header=None, names=['class', 'text'])\n",
    "kaggle_med['text'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[1]) \n",
    "kaggle_med['class'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == '7.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == '6.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train = kaggle_med.drop(kaggle_med[(kaggle_med['class'] == 'and') | (kaggle_med['class'] == '6.') | (kaggle_med['class'] == '7.')].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/original/kaggle_med/train.txt', new_df_train.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med = pd.read_csv('./data/original/kaggle_med/test.txt',delimiter='\\t', header=None, names=['class', 'text'])\n",
    "kaggle_med['text'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[1]) \n",
    "kaggle_med['class'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/kaggle_med/train.txt',delimiter='\\t', header=None, names=['class', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated()]\n",
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df['text'].duplicated()]\n",
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(duplicates)):\n",
    "    print(duplicates['text'].iloc[i])\n",
    "    print('===========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.groupby('text').apply(lambda x: pd.Series({\n",
    "    'class': list(x['class']),\n",
    "    'count': len(x)\n",
    "})).reset_index()\n",
    "duplicates = duplicates[duplicates.duplicated(subset='class')]\n",
    "for i in range(len(duplicates)):\n",
    "    count = duplicates['count'].iloc[i]\n",
    "    if count > 2:\n",
    "        print('text: ',duplicates['text'].iloc[i])\n",
    "        print('class: ',duplicates['class'].iloc[i])\n",
    "        print('count: ',duplicates['count'].iloc[i])\n",
    "        print('===========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/cardio/train.txt',delimiter='\\t', header=None, names=['class', 'text'])\n",
    "duplicates = df[df.duplicated()]\n",
    "len(duplicates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new session\n",
    "### augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',    \n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import snoop\n",
    "# @snoop(depth=2)\n",
    "def aug_samples(dataset_name, method_names, fraction, pct_words_to_swap, transformations_per_example, include_original):\n",
    "    data = load_data(dict_datasets[dataset_name])\n",
    "    print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "    #data = data.sample(frac=fraction)\n",
    "    #print(f'Sampled {len(data)} rows with fraction={fraction}')\n",
    "    for method in method_names:\n",
    "        if method not in dict_methods:\n",
    "            raise ValueError(f\"Method '{method}' is not supported\")\n",
    "        augmented_data = augment_text(\n",
    "            data,\n",
    "            dict_methods[method],\n",
    "            fraction=fraction,\n",
    "            pct_words_to_swap=pct_words_to_swap,\n",
    "            transformations_per_example=transformations_per_example,\n",
    "            label_column='class',\n",
    "            target_column='text',\n",
    "            include_original=include_original\n",
    "        )\n",
    "        augmented_data = augmented_data[['class', 'text']]\n",
    "        np.savetxt(\n",
    "            f'./data/augmented/{dataset_name}/{fraction}_{method}_{transformations_per_example}.txt',\n",
    "            augmented_data.values,\n",
    "            fmt='%s',\n",
    "            delimiter='\\t'\n",
    "        )\n",
    "        print(f'number of augmented samples: {len(augmented_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('agnews', ['eda'], 0.001, 0.5, 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_samples(dataset_name, method_names, fraction, pct_words_to_swap, transformations_per_example, include_original):\n",
    "    data = load_data(dict_datasets[dataset_name])\n",
    "    methods = method_names\n",
    "    for method in methods:\n",
    "        augmented_data = augment_text( data, method, fraction=fraction, pct_words_to_swap=pct_words_to_swap, transformations_per_example=transformations_per_example,\n",
    "                                                            label_column='class', target_column='text', include_original=include_original)\n",
    "        augmented_data = augmented_data[['class','text']]\n",
    "        np.savetxt(f'./data/augmented/{dataset_name}/{fraction}_{method}_{transformations_per_example}.txt', augmented_data.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('pc', ['checklist_augmenter'], 0.001, 0.5, 2, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dict_datasets['pc'])\n",
    "df = data.copy( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[~(df['text'].isin(['','none','nan','null','NaN','NULL','Null','None','NONE']))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/original/pc/train_clean.txt', df.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('pc_clean', ['checklist'], 0.001, 0.5, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/yelp/yelp.csv')\n",
    "df = df[['stars', 'text']]\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, \"text\"] = re.sub(r\"[\\\\']\", \"\", df.loc[i, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):    \n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\t+\", \" \", text)\n",
    "    text = re.sub(r\"\\r+\", \" \", text)\n",
    "    text = re.sub(r\"/\\sThe+\", \"The\", text)\n",
    "    text = re.sub(r\"[\\\\']+\", \"'\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train.to_csv('./data/original/yelp/train.csv', index=False)\n",
    "test.to_csv('./data/original/yelp/test.csv', index=False)\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x: clean_text(x))\n",
    "train['text'] = train['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "\n",
    "np.savetxt('./data/original/yelp/train.txt', train.values, fmt='%s', delimiter='\\t')\n",
    "np.savetxt('./data/original/yelp/test.txt', test.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./data/original/yelp/train.txt',delimiter='\\t', header=None, names=['class', 'text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new session\n",
    "### augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt',\n",
    "    'bbc': './data/original/bbc/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'embedding': 'embedding_augmenter',\n",
    "    'deletion': 'deletion_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_samples(dataset_name, method_names, fraction, pct_words_to_swap, transformations_per_example, include_original):\n",
    "    path = dict_datasets[dataset_name]\n",
    "    data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "    data = data.sample(frac=fraction)\n",
    "    print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "    #data = data.sample(frac=fraction)\n",
    "    #print(f'Sampled {len(data)} rows with fraction={fraction}')\n",
    "    for method in method_names:\n",
    "        if method not in dict_methods:\n",
    "            raise ValueError(f\"Method '{method}' is not supported\")\n",
    "        augmented_data = augment_text(\n",
    "            data,\n",
    "            dict_methods[method],\n",
    "            fraction=1,\n",
    "            pct_words_to_swap=pct_words_to_swap,\n",
    "            transformations_per_example=transformations_per_example,\n",
    "            label_column='class',\n",
    "            target_column='text',\n",
    "            include_original=include_original\n",
    "        )\n",
    "        augmented_data = augmented_data[['class', 'text']]\n",
    "        np.savetxt(\n",
    "            f'./data/augmented/{dataset_name}/{fraction}_{method}_{transformations_per_example}.txt',\n",
    "            augmented_data.values,\n",
    "            fmt='%s',\n",
    "            delimiter='\\t'\n",
    "        )\n",
    "        print(f'number of augmented samples: {len(augmented_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('yelp', ['eda'], 0.001, 0.5, 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/yelp/train.txt',delimiter='\\t', header=None, names=['class', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4284,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "from functions1 import *\n",
    "method_names = ['eda']\n",
    "dataset_name = 'yelp'\n",
    "fraction = 0.001\n",
    "path = dict_datasets[dataset_name]\n",
    "data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "data = data.sample(frac=fraction)\n",
    "print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'embedding': 'embedding_augmenter',\n",
    "    'deletion': 'deletion_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "from functions import *\n",
    "def aug_samples(name,frac):\n",
    "    path = dict_datasets[name]\n",
    "    data = load_data(path)\n",
    "    data = data.sample(frac=frac)\n",
    "    print(f'Loaded {len(data)} rows of data from {name}')\n",
    "    #methods = ['eda_augmenter','wordnet_augmenter','aeda_augmenter','backtranslation_augmenter']\n",
    "    methods = ['checklist_augmenter']\n",
    "    for method in methods:\n",
    "        augmented_data = augment_text(data, method,fraction=1,pct_words_to_swap=0.2 ,transformations_per_example=1,\n",
    "                    label_column='class',target_column='text',include_original=True)\n",
    "        augmented_data = augmented_data[['class','text']]\n",
    "        np.savetxt(f'./data/augmented/{name}/{method}_aug.txt', augmented_data.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('pc',0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "import logging\n",
    "from functions import *\n",
    "\n",
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}\n",
    "\n",
    "aug_num = [1,2,4]\n",
    "aug_frac = [0.1,0.2,0.5,1]\n",
    "\n",
    "logging.basicConfig(filename='augment.log', filemode='a', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "def aug_samples(name, frac):\n",
    "    path = dict_datasets[name]\n",
    "    data = load_data(path)\n",
    "    try:\n",
    "        data = data.sample(frac=frac)\n",
    "    except:\n",
    "        data = data.sample(n=1)\n",
    "    logging.info(f'Loaded {len(data)} rows of data from {name}')\n",
    "    \n",
    "    #methods = ['eda_augmenter','wordnet_augmenter','aeda_augmenter','backtranslation_augmenter']\n",
    "    methods = ['checklist_augmenter', 'eda_augmenter', 'wordnet_augmenter', 'aeda_augmenter', 'backtranslation_augmenter', 'clare_augmenter']\n",
    "    \n",
    "    for method in methods:\n",
    "        augmented_data = augment_text(data, method, fraction=1, pct_words_to_swap=0.2, transformations_per_example=1,\n",
    "                                      label_column='class', target_column='text', include_original=True)\n",
    "        augmented_data = augmented_data[['class','text']]\n",
    "        np.savetxt(f'./data/augmented/{name}/{method}_aug.txt', augmented_data.values, fmt='%s', delimiter='\\t')\n",
    "        logging.info(f'Augmented {len(augmented_data)} rows of data from {name} using {method} method')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('yelp', 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = dict_datasets['yelp']\n",
    "path = './data/original/yelp/train.csv'\n",
    "df = load_data(path)\n",
    "#df = pd.read_csv(path,  names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['text'].apply(lambda x: x.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(df['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads data from a txt file.\n",
    "    \"\"\"\n",
    "    # check file format\n",
    "    if path.endswith('.txt'):\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep='|', header=None, names=['text'])\n",
    "        except:\n",
    "            df = pd.read_csv(path, sep='\\t', header=None, names=['text'])\n",
    "        df['class'], df['text'] = zip(*df['text'].apply(lambda x: x.split('\\t', 1)))\n",
    "    else:\n",
    "        df = pd.read_csv(path, header=None, names=['class', 'text'])\n",
    "\n",
    "    return df[['class', 'text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/original/yelp/train.csv'\n",
    "df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads data from a txt file.\n",
    "    \"\"\"\n",
    "    # check file format\n",
    "    if path.endswith('.txt'):\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep='|', header=None, names=['text'])\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep='\\t', header=None, names=['text'])\n",
    "            except:\n",
    "                df = pd.read_csv(path, header=None, names=['text'])\n",
    "                df['class'] = df['text'].apply(lambda x: x.split(' ',1)[0])\n",
    "                df['text'] = df['text'].apply(lambda x: x.split(' ',1)[1])\n",
    "        else:\n",
    "            df['class'], df['text'] = zip(*df['text'].apply(lambda x: x.split('\\t', 1)))\n",
    "    else:\n",
    "        df = pd.read_csv(path, header=None, names=['class', 'text'],skiprows=1)\n",
    "\n",
    "    return df[['class', 'text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/original/bbc/train.csv'\n",
    "df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbc = pd.read_csv('./data/original/bbc/bbc.csv', header=None, names=['class', 'text'],skiprows=1)\n",
    "# bbc['class'] = bbc['class'].map({'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4})\n",
    "# train, test = train_test_split(bbc, test_size=0.1, random_state=42)\n",
    "\n",
    "# train.to_csv('./data/original/bbc/train.csv', index=False)\n",
    "# test.to_csv('./data/original/bbc/test.csv', index=False)\n",
    "\n",
    "# np.savetxt('./data/original/bbc/train.txt', train.values, fmt='%s', delimiter='\\t')\n",
    "# np.savetxt('./data/original/bbc/test.txt', test.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/original/bbc/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class'] = train_df['Category'].map({'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4})\n",
    "test_df['class'] = test_df['Category'].map({'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4})\n",
    "\n",
    "# train_df = train_df[['class', 'Text']]\n",
    "# test_df = test_df[['class', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):    \n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\t+\", \" \", text)\n",
    "    text = re.sub(r\"\\r+\", \" \", text)\n",
    "    text = re.sub(r\"/\\sThe+\", \"The\", text)\n",
    "    #text = re.sub(r\"(\\\\')+\", \"'\", text)\n",
    "    text = re.sub(r'\\\\+', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_csv('./data/original/yelp/yelp.csv')\n",
    "yelp = yelp[['stars', 'text']]\n",
    "yelp['text'] = yelp['text'].map(clean_text)\n",
    "yelp.columns = ['class', 'text']\n",
    "yelp['text'] = yelp['text'].apply(lambda x: str(x).replace(\"\\\\'s\",\"'s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(yelp, test_size=0.1, random_state=42)\n",
    "train.to_csv('./data/original/bbc/train.csv', index=False)\n",
    "test.to_csv('./data/original/bbc/test.csv', index=False)\n",
    "\n",
    "np.savetxt('./data/original/bbc/train.txt', train.values, fmt='%s', delimiter='\\t')\n",
    "np.savetxt('./data/original/bbc/test.txt', test.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (10):\n",
    "    print(yelp['class'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].iloc[2248]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'There are a few nice semi-upscale restaurants here. It\\'s also got a really good swanky (and somewhat hidden) bar here.'\n",
    "new_txt  = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment\n",
    "aug_samples('yelp', 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_samples(dataset_name, method_names, fraction, pct_words_to_swap, transformations_per_example, include_original):\n",
    "    data = load_data(dict_datasets[dataset_name])\n",
    "    print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "    #data = data.sample(frac=fraction)\n",
    "    #print(f'Sampled {len(data)} rows with fraction={fraction}')\n",
    "    for method in method_names:\n",
    "        if method not in dict_methods:\n",
    "            raise ValueError(f\"Method '{method}' is not supported\")\n",
    "        augmented_data = augment_text(\n",
    "            data,\n",
    "            dict_methods[method],\n",
    "            fraction=fraction,\n",
    "            pct_words_to_swap=pct_words_to_swap,\n",
    "            transformations_per_example=transformations_per_example,\n",
    "            label_column='class',\n",
    "            target_column='text',\n",
    "            include_original=include_original\n",
    "        )\n",
    "        augmented_data = augmented_data[['class', 'text']]\n",
    "\n",
    "        print(f'number of augmented samples: {len(augmented_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "import logging\n",
    "from functions import *\n",
    "def test_augmentation_methods():\n",
    "    # Define the input paths for the datasets\n",
    "    dict_datasets = {\n",
    "        'agnews': './data/original/agnews/train.txt',\n",
    "        'cardio': './data/original/cardio/train.txt',\n",
    "        'cr': './data/original/cr/train.txt',\n",
    "        'pc': './data/original/pc/train.txt',\n",
    "        'pc_clean': './data/original/pc/train_clean.txt',\n",
    "        'sst2': './data/original/sst2/train.txt',\n",
    "        'subj': './data/original/subj/train.txt',\n",
    "        'trec': './data/original/trec/train.txt',\n",
    "        'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "        'yelp': './data/original/yelp/train.txt',\n",
    "        'bbc': './data/original/bbc/train.txt'\n",
    "    }\n",
    "\n",
    "    # Define the augmentation methods and their names\n",
    "    dict_methods = {\n",
    "        'aeda': 'aeda_augmenter',\n",
    "        'backtranslation': 'backtranslation_augmenter',\n",
    "        'checklist': 'checklist_augmenter',\n",
    "        'clare': 'clare_augmenter',\n",
    "        'eda': 'eda_augmenter',\n",
    "        'embedding': 'embedding_augmenter',\n",
    "        'deletion': 'deletion_augmenter',\n",
    "        'wordnet': 'wordnet_augmenter'\n",
    "    }\n",
    "\n",
    "    datasets_with_samples = []\n",
    "    methods_with_samples = []\n",
    "\n",
    "    for dataset_name in dict_datasets:\n",
    "        path = dict_datasets[dataset_name]\n",
    "        data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "        print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "\n",
    "        # Take only one sample from each dataset\n",
    "        data = data.sample(n=1)\n",
    "\n",
    "        for method in dict_methods:\n",
    "            augmented_data = augment_text(\n",
    "                data,\n",
    "                dict_methods[method],\n",
    "                fraction=1,\n",
    "                pct_words_to_swap=0.1,\n",
    "                transformations_per_example=1,\n",
    "                label_column='class',\n",
    "                target_column='text',\n",
    "                include_original=False\n",
    "            )\n",
    "            augmented_data = augmented_data[['class', 'text']]\n",
    "            \n",
    "            if len(augmented_data) > 0:\n",
    "                print(f\"Generated {len(augmented_data)} samples for dataset '{dataset_name}' and method '{method}'\")\n",
    "                datasets_with_samples.append(dataset_name)\n",
    "                methods_with_samples.append(method)\n",
    "    \n",
    "    datasets_with_samples = set(datasets_with_samples)\n",
    "    methods_with_samples = set(methods_with_samples)\n",
    "    \n",
    "    print(f\"\\nDatasets with samples: {', '.join(datasets_with_samples)}\")\n",
    "    print(f\"Methods with samples: {', '.join(methods_with_samples)}\")\n",
    "    print(f\"\\nDatasets without any samples: {', '.join(set(dict_datasets.keys()) - datasets_with_samples)}\")\n",
    "    print(f\"Methods without any samples: {', '.join(set(dict_methods.keys()) - methods_with_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "import logging\n",
    "from functions import *\n",
    "def test_augmentation_methods():\n",
    "    # Define the input paths for the datasets\n",
    "    dict_datasets = {\n",
    "        'agnews': './data/original/agnews/train.txt',\n",
    "        'cardio': './data/original/cardio/train.txt',\n",
    "        'cr': './data/original/cr/train.txt',\n",
    "        'pc': './data/original/pc/train.txt',\n",
    "        'pc_clean': './data/original/pc/train_clean.txt',\n",
    "        'sst2': './data/original/sst2/train.txt',\n",
    "        'subj': './data/original/subj/train.txt',\n",
    "        'trec': './data/original/trec/train.txt',\n",
    "        'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "        'yelp': './data/original/yelp/train.txt',\n",
    "        'bbc': './data/original/bbc/train.txt'\n",
    "    }\n",
    "\n",
    "    # Define the augmentation methods and their names\n",
    "    dict_methods = {\n",
    "        'aeda': 'aeda_augmenter',\n",
    "        'backtranslation': 'backtranslation_augmenter',\n",
    "        'checklist': 'checklist_augmenter',\n",
    "        'clare': 'clare_augmenter',\n",
    "        'eda': 'eda_augmenter',\n",
    "        'wordnet': 'wordnet_augmenter'\n",
    "    }\n",
    "\n",
    "    datasets_with_samples = []\n",
    "    methods_with_samples = []\n",
    "    results = []\n",
    "\n",
    "    for dataset_name in dict_datasets:\n",
    "        path = dict_datasets[dataset_name]\n",
    "        data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "        print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "\n",
    "        # Take only one sample from each dataset\n",
    "        data = data.sample(n=1)\n",
    "        print(data)\n",
    "\n",
    "        for method in dict_methods:\n",
    "            print('=====================================')\n",
    "            print(f\"Augmenting dataset '{dataset_name}' with method '{method}'\")\n",
    "            print('=====================================')\n",
    "            augmented_data = augment_text(\n",
    "                data,\n",
    "                dict_methods[method],\n",
    "                fraction=1,\n",
    "                pct_words_to_swap=0.1,\n",
    "                transformations_per_example=1,\n",
    "                label_column='class',\n",
    "                target_column='text',\n",
    "                include_original=False\n",
    "            )\n",
    "            augmented_data = augmented_data[['class', 'text']]\n",
    "            \n",
    "            if len(augmented_data) > 0:\n",
    "                print(f\"Generated {len(augmented_data)} samples for dataset '{dataset_name}' and method '{method}'\")\n",
    "                datasets_with_samples.append(dataset_name)\n",
    "                methods_with_samples.append(method)\n",
    "                result = {\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'method_name': method,\n",
    "                    'num_samples': len(augmented_data)\n",
    "                }\n",
    "                results.append(result)\n",
    "    \n",
    "    datasets_with_samples = set(datasets_with_samples)\n",
    "    methods_with_samples = set(methods_with_samples)\n",
    "    \n",
    "    print(f\"\\nDatasets with samples: {', '.join(datasets_with_samples)}\")\n",
    "    print(f\"Methods with samples: {', '.join(methods_with_samples)}\")\n",
    "    print(f\"\\nDatasets without any samples: {', '.join(set(dict_datasets.keys()) - datasets_with_samples)}\")\n",
    "    print(f\"Methods without any samples: {', '.join(set(dict_methods.keys()) - methods_with_samples)}\")\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "import sys\n",
    "\n",
    "# Redirect stderr to /dev/null\n",
    "sys.stderr = open('/dev/null', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_augmentation_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "def clare_augmentation(text, pct_words_to_swap, transformations_per_example):\n",
    "    # Load pre-trained masked language model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Replace a fraction of tokens with [MASK] tokens\n",
    "    num_words_to_swap = int(pct_words_to_swap * len(tokens))\n",
    "    indices_to_swap = set()\n",
    "    while len(indices_to_swap) < num_words_to_swap:\n",
    "        index = torch.randint(len(tokens), size=(1,)).item()\n",
    "        if index not in indices_to_swap:\n",
    "            indices_to_swap.add(index)\n",
    "            tokens[index] = \"[MASK]\"\n",
    "\n",
    "    # Generate transformed text examples\n",
    "    transformed_texts = set()\n",
    "    for i in range(transformations_per_example):\n",
    "        # Mask some tokens\n",
    "        masked_tokens = tokens.copy()\n",
    "        for index in indices_to_swap:\n",
    "            masked_tokens[index] = \"[MASK]\"\n",
    "\n",
    "        # Convert tokens to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "\n",
    "        # Get predicted token probabilities from model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            predictions = outputs[0][0]\n",
    "\n",
    "        # Generate new tokens by sampling from predicted probabilities\n",
    "        new_tokens = []\n",
    "        for index in indices_to_swap:\n",
    "            probabilities = predictions[index]\n",
    "            probabilities /= probabilities.sum()\n",
    "            predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "            new_tokens.append(predicted_token)\n",
    "        for index, token in zip(indices_to_swap, new_tokens):\n",
    "            masked_tokens[index] = token\n",
    "\n",
    "        # Convert tokens back to text\n",
    "        transformed_text = tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "        transformed_texts.add(transformed_text)\n",
    "\n",
    "    return list(transformed_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/original/agnews/train.txt', delimiter='\\t', header=None, names=['class', 'text'])\n",
    "data = data.sample(n=1)['text'].values[0]\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clare_augmentation(data, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "def clare_augmentation(text, pct_words_to_swap, transformations_per_example):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.BertForMaskedLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess text\n",
    "    text = text.replace('#', '')  # Remove special characters\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Select indices of words to perturb\n",
    "    num_words = len(token_ids)\n",
    "    indices_to_modify = set()\n",
    "    while len(indices_to_modify) < int(num_words * pct_words_to_swap):\n",
    "        indices_to_modify.add(torch.randint(num_words, size=(1,)).item())\n",
    "\n",
    "    # Perturb words\n",
    "    new_texts = []\n",
    "    for _ in range(transformations_per_example):\n",
    "        new_tokens = []\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if i in indices_to_modify:\n",
    "                # Replace token with predicted token from BERT model\n",
    "                predictions = model(torch.tensor([token_ids])).logits[0]\n",
    "                probabilities = predictions[i]\n",
    "                probabilities /= probabilities.sum()\n",
    "                predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                new_tokens.append(predicted_token)\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "\n",
    "        new_text = tokenizer.convert_tokens_to_string(new_tokens)\n",
    "        new_texts.append(new_text)\n",
    "\n",
    "    return new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Not too many 27-year-old rookies come down the pike \\\n",
    "    in the NBA. But Nets coach Lawrence Frank wasn #39;t concerned \\\n",
    "      that Awvee Storey #39;s only previous NBA experience was a summer camp with the Dallas Mavericks.\"\n",
    "augmented_texts = clare_augmentation(text, pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "print(augmented_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Not too many 27-year-old rookies come down the pike in the NBA. But Nets coach Lawrence Frank wasn #39;t concerned that Awvee Storey #39;s only previous NBA experience was a summer camp with the Dallas Mavericks.\"\n",
    "augmented_texts = clare_augmentation(text, pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "print(augmented_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Not too many 27-year-old rookies come down the pike in the NBA. But Nets coach Lawrence Frank wasn #39;t concerned that Awvee Storey #39;s only previous NBA experience was a summer camp with the Dallas Mavericks.\"\n",
    "augmented_texts = clare_augmentation(text, pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "print(augmented_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "augmented_texts = clare_augmentation(text, pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "print(augmented_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "augmented_texts = clare_augmentation(text,model_name='bert-large-cased', pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "\n",
    "print(augmented_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functions import *\n",
    "\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    #'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt',\n",
    "    'bbc': './data/original/bbc/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    #'embedding': 'embedding_augmenter',\n",
    "    #'deletion': 'deletion_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(dict_datasets['yelp'])\n",
    "df = df.sample(frac=0.01, random_state=42)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clare\n",
    "aug = clare.Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "for text in df['text']:\n",
    "    print(aug.augment(text))\n",
    "# res = aug.augment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clare\n",
    "aug = clare.Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=5)\n",
    "for text in df['text']:\n",
    "    print(aug.augment(text))\n",
    "# res = aug.augment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functions import *\n",
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(frac=0.0001, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = augment_text(df,'clare_augmenter',fraction=1, pct_words_to_swap=0.5, transformations_per_example=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clare import Clare_Augmenter\n",
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(frac=0.0001, random_state=42)\n",
    "sentence = df['text'].iloc[0]\n",
    "aug = Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "aug_df = aug.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "class Clare_Augmenter:\n",
    "    def __init__(self, num_labels=2, pct_words_to_swap=0.1, transformations_per_example=1):\n",
    "        self.pct_words_to_swap = pct_words_to_swap\n",
    "        self.transformations_per_example = transformations_per_example\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def augment(self,text,model_name='bert-base-uncased'):\n",
    "     \n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=self.num_labels)\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        new_texts = []\n",
    "        for _ in range(self.transformations_per_example):\n",
    "            new_tokens = []\n",
    "            for i, token in enumerate(tokens):\n",
    "                # Decide whether to perturb this token\n",
    "                if torch.rand(1).item() < self.pct_words_to_swap:\n",
    "                    # Replace the token with a random token from the BERT vocabulary\n",
    "                    masked_tokens = [\"[MASK]\"] * len(tokens)\n",
    "                    masked_tokens[i] = token\n",
    "                    masked_input = \" \".join(masked_tokens)\n",
    "                    input_ids = tokenizer.encode(masked_input, return_tensors=\"pt\")\n",
    "                    with torch.no_grad():\n",
    "                        predictions = model(input_ids)[0][0, i].reshape(-1)  # reshape predictions\n",
    "                    probabilities = predictions.softmax(dim=0)\n",
    "                    predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                    predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                    new_tokens.append(predicted_token)\n",
    "                else:\n",
    "                    new_tokens.append(token)\n",
    "            \n",
    "            # Convert the new tokens back to text and add it to the list of augmented texts\n",
    "            new_text = tokenizer.convert_tokens_to_string(new_tokens)\n",
    "            new_texts.append(new_text)\n",
    "        \n",
    "        return new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(frac=0.0001, random_state=42)\n",
    "sentence = df['text'].iloc[0]\n",
    "aug = Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "aug_df = aug.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clare_Augmenter:\n",
    "    def __init__(self, num_labels=2, pct_words_to_swap=0.1, transformations_per_example=1):\n",
    "        self.pct_words_to_swap = pct_words_to_swap\n",
    "        self.transformations_per_example = transformations_per_example\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def augment(self,text,model_name='bert-base-uncased'):\n",
    "     \n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name, max_length=1024)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=self.num_labels)\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        new_texts = []\n",
    "        for _ in range(self.transformations_per_example):\n",
    "            new_tokens = []\n",
    "            for i, token in enumerate(tokens):\n",
    "                # Decide whether to perturb this token\n",
    "                if torch.rand(1).item() < self.pct_words_to_swap:\n",
    "                    # Replace the token with a random token from the BERT vocabulary\n",
    "                    masked_tokens = [\"[MASK]\"] * len(tokens)\n",
    "                    masked_tokens[i] = token\n",
    "                    masked_input = \" \".join(masked_tokens)\n",
    "                    input_ids = tokenizer.encode(masked_input, return_tensors=\"pt\")\n",
    "                    with torch.no_grad():\n",
    "                        predictions = model(input_ids)[0][0, i].reshape(-1)  # reshape predictions\n",
    "                    probabilities = predictions.softmax(dim=0)\n",
    "                    predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                    predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                    new_tokens.append(predicted_token)\n",
    "                else:\n",
    "                    new_tokens.append(token)\n",
    "            \n",
    "            # Convert the new tokens back to text and add it to the list of augmented texts\n",
    "            new_text = tokenizer.convert_tokens_to_string(new_tokens)\n",
    "            new_texts.append(new_text)\n",
    "        \n",
    "        return new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(frac=0.0001, random_state=42)\n",
    "sentence = df['text'].iloc[0]\n",
    "aug = Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "aug_df = aug.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clare_Augmenter:\n",
    "    def __init__(self, pct_words_to_swap=0.1, transformations_per_example=1):\n",
    "        self.pct_words_to_swap = pct_words_to_swap\n",
    "        self.transformations_per_example = transformations_per_example\n",
    "        \n",
    "        # Initialize the tokenizer and model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def augment(self, text):\n",
    "        # Tokenize the input text\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        new_texts = []\n",
    "        for _ in range(self.transformations_per_example):\n",
    "            new_tokens = []\n",
    "            for i, token in enumerate(tokens):\n",
    "                # Decide whether to perturb this token\n",
    "                if torch.rand(1).item() < self.pct_words_to_swap:\n",
    "                    # Replace the token with a random token from the BERT vocabulary\n",
    "                    masked_tokens = [\"[MASK]\"] * len(tokens)\n",
    "                    masked_tokens[i] = token\n",
    "                    masked_input = \" \".join(masked_tokens)\n",
    "                    input_ids = self.tokenizer.encode(masked_input, return_tensors=\"pt\")\n",
    "                    with torch.no_grad():\n",
    "                        predictions = self.model(input_ids).logits[0, i].reshape(-1)  # reshape predictions\n",
    "                    probabilities = predictions.softmax(dim=0)\n",
    "                    predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                    predicted_token = self.tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                    new_tokens.append(predicted_token)\n",
    "                else:\n",
    "                    new_tokens.append(token)\n",
    "            \n",
    "            # Convert the new tokens back to text and add it to the list of augmented texts\n",
    "            new_text = self.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "            new_texts.append(new_text)\n",
    "        \n",
    "        return new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(frac=0.0001, random_state=42)\n",
    "sentence = df['text'].iloc[0]\n",
    "aug = Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "aug_df = aug.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clare_Augmenter:\n",
    "    def __init__(self, pct_words_to_swap=0.1, transformations_per_example=1):\n",
    "        self.pct_words_to_swap = pct_words_to_swap\n",
    "        self.transformations_per_example = transformations_per_example\n",
    "        \n",
    "        # Initialize the tokenizer and model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", max_length=1024)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def augment(self, text):\n",
    "        # Tokenize the input text\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        new_texts = []\n",
    "        for _ in range(self.transformations_per_example):\n",
    "            new_tokens = []\n",
    "            for i, token in enumerate(tokens):\n",
    "                # Decide whether to perturb this token\n",
    "                if torch.rand(1).item() < self.pct_words_to_swap:\n",
    "                    # Replace the token with a random token from the BERT vocabulary\n",
    "                    masked_tokens = [\"[MASK]\"] * len(tokens)\n",
    "                    masked_tokens[i] = token\n",
    "                    masked_input = \" \".join(masked_tokens)\n",
    "                    input_ids = self.tokenizer.encode(masked_input, return_tensors=\"pt\")\n",
    "                    with torch.no_grad():\n",
    "                        predictions = self.model(input_ids).logits[0, i].reshape(-1)  # reshape predictions\n",
    "                    probabilities = predictions.softmax(dim=0)\n",
    "                    predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                    predicted_token = self.tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                    new_tokens.append(predicted_token)\n",
    "                else:\n",
    "                    new_tokens.append(token)\n",
    "            \n",
    "            # Convert the new tokens back to text and add it to the list of augmented texts\n",
    "            new_text = self.tokenizer.convert_tokens_to_string(new_tokens)\n",
    "            new_texts.append(new_text)\n",
    "        \n",
    "        return new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(frac=0.0001, random_state=42)\n",
    "sentence = df['text'].iloc[0]\n",
    "aug = Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "aug_df = aug.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clare_augmentation(text, pct_words_to_swap, transformations_per_example, max_length=512):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.BertForMaskedLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess text\n",
    "    text = text.replace('#', '')  # Remove special characters\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt').input_ids[0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "    # Select indices of words to perturb\n",
    "    num_words = len(tokens)\n",
    "    indices_to_modify = set()\n",
    "    while len(indices_to_modify) < int(num_words * pct_words_to_swap):\n",
    "        indices_to_modify.add(torch.randint(num_words, size=(1,)).item())\n",
    "\n",
    "    # Perturb words\n",
    "    new_texts = []\n",
    "    for _ in range(transformations_per_example):\n",
    "        new_tokens = []\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            if i in indices_to_modify:\n",
    "                # Replace token with predicted token from BERT model\n",
    "                predictions = model(torch.tensor([tokens])).logits[0]\n",
    "                probabilities = predictions[i]\n",
    "                probabilities /= probabilities.sum()\n",
    "                predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                new_tokens.append(predicted_token)\n",
    "            else:\n",
    "                new_tokens.append(tokenizer.convert_ids_to_tokens([token_id], skip_special_tokens=True)[0])\n",
    "\n",
    "\n",
    "        new_text = tokenizer.convert_tokens_to_string(new_tokens)\n",
    "        new_texts.append(new_text)\n",
    "\n",
    "    return new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'The quick brown fox jumps over the lazy dog.'\n",
    "clare_augmentation(txt, pct_words_to_swap=0.5, transformations_per_example=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "def clare_augmentation(text, pct_words_to_swap, transformations_per_example):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.BertForMaskedLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess text\n",
    "    text = text.replace('#', '')  # Remove special characters\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Select indices of words to perturb\n",
    "    num_words = len(token_ids)\n",
    "    indices_to_modify = set()\n",
    "    while len(indices_to_modify) < int(num_words * pct_words_to_swap):\n",
    "        indices_to_modify.add(torch.randint(num_words, size=(1,)).item())\n",
    "\n",
    "    # Perturb words\n",
    "    new_texts = []\n",
    "    for _ in range(transformations_per_example):\n",
    "        new_tokens = []\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if i in indices_to_modify:\n",
    "                # Replace token with predicted token from BERT model\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(torch.tensor([token_ids])).logits[0]\n",
    "                    probabilities = predictions[i].softmax(dim=0)\n",
    "                    predicted_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                    predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "                    new_tokens.append(predicted_token)\n",
    "            else:\n",
    "                new_tokens.append(tokenizer.convert_ids_to_tokens([token_id])[0])\n",
    "\n",
    "        new_text = tokenizer.convert_tokens_to_string(new_tokens)\n",
    "        new_texts.append(new_text)\n",
    "\n",
    "    return new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'The quick brown fox jumps over the lazy dog.'\n",
    "clare_augmentation(txt, pct_words_to_swap=0.5, transformations_per_example=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functions import *\n",
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(frac=0.0001, random_state=42)\n",
    "sentence = df['text'].iloc[0]\n",
    "aug = Clare_Augmenter(pct_words_to_swap=0.5, transformations_per_example=1)\n",
    "aug_df = aug.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "import logging\n",
    "from functions import *\n",
    "def test_augmentation_methods():\n",
    "    # Define the input paths for the datasets\n",
    "    dict_datasets = {\n",
    "        'agnews': './data/original/agnews/train.txt',\n",
    "        # 'cardio': './data/original/cardio/train.txt',\n",
    "        # 'cr': './data/original/cr/train.txt',\n",
    "        # 'pc': './data/original/pc/train.txt',\n",
    "        # 'pc_clean': './data/original/pc/train_clean.txt',\n",
    "        # 'sst2': './data/original/sst2/train.txt',\n",
    "        # 'subj': './data/original/subj/train.txt',\n",
    "        # 'trec': './data/original/trec/train.txt',\n",
    "        # 'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "        # 'yelp': './data/original/yelp/train.txt',\n",
    "        # 'bbc': './data/original/bbc/train.txt'\n",
    "    }\n",
    "\n",
    "    # Define the augmentation methods and their names\n",
    "    dict_methods = {\n",
    "        # 'aeda': 'aeda_augmenter',\n",
    "        # 'backtranslation': 'backtranslation_augmenter',\n",
    "        # 'checklist': 'checklist_augmenter',\n",
    "         'clare': 'clare_augmenter',\n",
    "        # 'eda': 'eda_augmenter',\n",
    "        # 'wordnet': 'wordnet_augmenter'\n",
    "    }\n",
    "\n",
    "    datasets_with_samples = []\n",
    "    methods_with_samples = []\n",
    "    results = []\n",
    "\n",
    "    for dataset_name in dict_datasets:\n",
    "        path = dict_datasets[dataset_name]\n",
    "        data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "        print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "\n",
    "        # Take only one sample from each dataset\n",
    "        data = data.sample(n=1)\n",
    "        print(data)\n",
    "\n",
    "        for method in dict_methods:\n",
    "            print('=====================================')\n",
    "            print(f\"Augmenting dataset '{dataset_name}' with method '{method}'\")\n",
    "            print('=====================================')\n",
    "            augmented_data = augment_text(\n",
    "                data,\n",
    "                dict_methods[method],\n",
    "                fraction=1,\n",
    "                pct_words_to_swap=0.1,\n",
    "                transformations_per_example=1,\n",
    "                label_column='class',\n",
    "                target_column='text',\n",
    "                include_original=False\n",
    "            )\n",
    "            augmented_data = augmented_data[['class', 'text']]\n",
    "            \n",
    "            if len(augmented_data) > 0:\n",
    "                print(f\"Generated {len(augmented_data)} samples for dataset '{dataset_name}' and method '{method}'\")\n",
    "                datasets_with_samples.append(dataset_name)\n",
    "                methods_with_samples.append(method)\n",
    "                result = {\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'method_name': method,\n",
    "                    'num_samples': len(augmented_data)\n",
    "                }\n",
    "                results.append(result)\n",
    "    \n",
    "    datasets_with_samples = set(datasets_with_samples)\n",
    "    methods_with_samples = set(methods_with_samples)\n",
    "    \n",
    "    print(f\"\\nDatasets with samples: {', '.join(datasets_with_samples)}\")\n",
    "    print(f\"Methods with samples: {', '.join(methods_with_samples)}\")\n",
    "    print(f\"\\nDatasets without any samples: {', '.join(set(dict_datasets.keys()) - datasets_with_samples)}\")\n",
    "    print(f\"Methods without any samples: {', '.join(set(dict_methods.keys()) - methods_with_samples)}\")\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "import sys\n",
    "\n",
    "# Redirect stderr to /dev/null\n",
    "sys.stderr = open('/dev/null', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_augmentation_methods()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "import logging\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    #'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt',\n",
    "    'bbc': './data/original/bbc/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter',\n",
    "    'charswap': 'charswap_augmenter',\n",
    "    'deletion': 'deletion_augmenter',\n",
    "    'embedding': 'embedding_augmenter'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('./data/original/agnews/train.txt')\n",
    "df = df.sample(n=1, random_state=42)\n",
    "method = dict_methods['backtranslation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "res = augment_text(df, method, fraction=1, pct_words_to_swap=0.5, transformations_per_example=1, label_column='class', target_column='text', include_original=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dict_methods:\n",
    "    for j in dict_datasets:\n",
    "        df = load_data(dict_datasets[j])\n",
    "        df = df.sample(n=1)        \n",
    "        method = dict_methods[i]\n",
    "        print(f'augmenting {j} with {i} \\n ')\n",
    "        res = augment_text(df, method, fraction=1, pct_words_to_swap=0.5, transformations_per_example=1, label_column='class', target_column='text', include_original=True)\n",
    "        print('=====================================')\n",
    "        print(f'results for {j} with {i}: \\n {res}')\n",
    "        print('=====================================')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert all txt files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/test.txt',\n",
    "    'cardio': './data/original/cardio/test.txt',\n",
    "    'cr': './data/original/cr/test.txt',\n",
    "    'pc': './data/original/pc/test.txt',\n",
    "    #'pc_clean': './data/original/pc/test_clean.txt',\n",
    "    'sst2': './data/original/sst2/test.txt',\n",
    "    'subj': './data/original/subj/test.txt',\n",
    "    'trec': './data/original/trec/test.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/test.txt',\n",
    "    'yelp': './data/original/yelp/test.txt',\n",
    "    'bbc': './data/original/bbc/test.txt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all datasets to csv if it is not already existing\n",
    "for i in dict_datasets:\n",
    "    path = dict_datasets[i]\n",
    "    if os.path.exists(f'./data/original/{i}/test.csv'):\n",
    "        print(f'./data/original/{i}/test.csv already exists')\n",
    "    else:\n",
    "        print(f'./data/original/{i}/test.csv does not exist')\n",
    "        print(f'creating ./data/original/{i}/test.csv')\n",
    "        df = load_data(path)\n",
    "        df.to_csv(f'./data/original/{i}/test.csv', index=False)\n",
    "        print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_csv(txt_file):\n",
    "    '''\n",
    "    Return a csv file from a txt file with the same name\n",
    "    with class and text separated by a comma.\n",
    "    '''\n",
    "    if txt_file.endswith('.txt'):\n",
    "        try:\n",
    "            df = pd.read_csv(txt_file, sep='\\t', header=None, names=['class', 'text'])\n",
    "        except pd.errors.ParserError:\n",
    "            df = pd.read_csv(txt_file, sep=' ', header=None, names=['class', 'text'], engine='python')\n",
    "        df.to_csv(txt_file[:-4] + '.csv', index=False)\n",
    "    else:\n",
    "        raise ValueError('File format not supported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = './data/original/cr/test.txt'\n",
    "df = pd.read_csv(txt_file, sep='\\t', header=None, names=['class', 'text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data('./data/original/cr/test.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check all datasets with methods with csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "import logging\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.csv',\n",
    "    'cardio': './data/original/cardio/train.csv',\n",
    "    'cr': './data/original/cr/train.csv',\n",
    "    'pc': './data/original/pc/train.csv',\n",
    "    'sst2': './data/original/sst2/train.csv',\n",
    "    'subj': './data/original/subj/train.csv',\n",
    "    'trec': './data/original/trec/train.csv',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.csv',\n",
    "    'yelp': './data/original/yelp/train.csv',\n",
    "    'bbc': './data/original/bbc/train.csv'\n",
    "}\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter',\n",
    "    'charswap': 'charswap_augmenter',\n",
    "    'deletion': 'deletion_augmenter',\n",
    "    'embedding': 'embedding_augmenter'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmenting agnews with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for agnews with aeda: \n",
      "                                                 text  class\n",
      "0  AFP - Women activists urged Sri Lankan authori...      1\n",
      "1  . AFP ? - Women : activists urged ! Sri , Lank...      1\n",
      "=====================================\n",
      "augmenting cardio with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cardio with aeda: \n",
      "                                                 text  class\n",
      "0   This study compares the metabolism and functi...     14\n",
      "1   This . study compares : the metabolism ! and ...     14\n",
      "=====================================\n",
      "augmenting cr with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cr with aeda: \n",
      "                                                 text  class\n",
      "0  my jpeg pictures are viewable ( not so sharp a...      0\n",
      "1  my jpeg pictures are viewable ( not so sharp a...      0\n",
      "=====================================\n",
      "augmenting pc with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for pc with aeda: \n",
      "                     text  class\n",
      "0    can't think of one.      0\n",
      "1  can't think ; of one.      0\n",
      "=====================================\n",
      "augmenting sst2 with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for sst2 with aeda: \n",
      "                                                 text  class\n",
      "0  shot in rich , shadowy black-and-white , devil...      1\n",
      "1  ; shot in rich , shadowy , black-and-white , d...      1\n",
      "=====================================\n",
      "augmenting subj with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for subj with aeda: \n",
      "                                                 text  class\n",
      "0   with one exception , every blighter in this p...      0\n",
      "1  :  with one exception , every blighter in this...      0\n",
      "=====================================\n",
      "augmenting trec with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for trec with aeda: \n",
      "                                                 text  class\n",
      "0         What vowel do all Esperanto nouns end in ?      1\n",
      "1  What vowel : do all ! Esperanto nouns ! end . ...      1\n",
      "=====================================\n",
      "augmenting kaggle_med with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for kaggle_med with aeda: \n",
      "                                                 text  class\n",
      "0   Plantar fascitis/heel spur syndrome.  The pat...     35\n",
      "1  ;  Plantar fascitis/heel spur syndrome.  The p...     35\n",
      "=====================================\n",
      "augmenting yelp with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for yelp with aeda: \n",
      "                                                 text  class\n",
      "0  What an incredible little hidden gem!  I have ...      4\n",
      "1  What . an incredible little , hidden gem!  I h...      4\n",
      "=====================================\n",
      "augmenting bbc with aeda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for bbc with aeda: \n",
      "                                                 text  class\n",
      "0  In our continuing quest to identify cool, loca...      4\n",
      "1  In our ! continuing quest to , identify cool, ...      4\n",
      "=====================================\n",
      "augmenting agnews with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for agnews with backtranslation: \n",
      "                                                 text  class\n",
      "0  NEW DELHI: IBMs PC business sale could rock th...      4\n",
      "1  New Delhi: IBM PCs can shake the page of PC In...      4\n",
      "=====================================\n",
      "augmenting cardio with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for cardio with backtranslation: \n",
      "                                                 text  class\n",
      "0   Isogenic pairs of env deletion clones were pr...     23\n",
      "1  Removal of Imesh clones occupied with or witho...     23\n",
      "=====================================\n",
      "augmenting cr with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for cr with backtranslation: \n",
      "                                                 text  class\n",
      "0  it seems to be working fine , but we 've not u...      1\n",
      "1   If we operate well, if we don't get out of here.      1\n",
      "=====================================\n",
      "augmenting pc with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for pc with backtranslation: \n",
      "                text  class\n",
      "0  Hey NICE printer      1\n",
      "1      NICE Printer      1\n",
      "=====================================\n",
      "augmenting sst2 with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for sst2 with backtranslation: \n",
      "                                                 text  class\n",
      "0  it 's a film that 's destined to win a wide su...      1\n",
      "1  It was not possible to choose the estimated \"b...      1\n",
      "=====================================\n",
      "augmenting subj with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for subj with backtranslation: \n",
      "                                                 text  class\n",
      "0  sid , a sloth who never stops talking is left ...      1\n",
      "1  VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV...      1\n",
      "=====================================\n",
      "augmenting trec with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for trec with backtranslation: \n",
      "                   text  class\n",
      "0    What makes salt ?      0\n",
      "1  What are you doing?      0\n",
      "=====================================\n",
      "augmenting kaggle_med with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for kaggle_med with backtranslation: \n",
      "                                                 text  class\n",
      "0   Dementia and aortoiliac occlusive disease bil...      3\n",
      "1  Operation aortobifemoral and bilateral blocked...      3\n",
      "=====================================\n",
      "augmenting yelp with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for yelp with backtranslation: \n",
      "                                                 text  class\n",
      "0  Tim is really a very friendly and honest guy. ...      5\n",
      "1  And when you say it, and when you say it, you ...      5\n",
      "=====================================\n",
      "augmenting bbc with backtranslation \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for bbc with backtranslation: \n",
      "                                                 text  class\n",
      "0  Just off University, but manages to feel like ...      5\n",
      "1  On the other hand, in high school, he listened...      5\n",
      "=====================================\n",
      "augmenting agnews with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/huggingface_hub/file_download.py:621: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-05 21:29:23,205 loading file /home/peyman/.flair/models/ner-english/4f4cdab26f24cb98b732b389e6cebc646c36f54cfd6e0b7d3b90b25656e4262f.8baa8ae8795f4df80b28e7f7b61d788ecbb057d1dc85aacb316f1bd02837a4a4\n",
      "2023-03-05 21:29:24,378 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "=====================================\n",
      "results for agnews with checklist: \n",
      "                                                 text  class\n",
      "0  Indonesian authorities on Thursday ordered a c...      1\n",
      "1  Indonesian authorities on Thursday ordered a c...      1\n",
      "=====================================\n",
      "augmenting cardio with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cardio with checklist: \n",
      "                                                 text  class\n",
      "0  Recurrent classic migraine attacks following t...     23\n",
      "1  Recurrent classic migraine attacks following t...     23\n",
      "=====================================\n",
      "augmenting cr with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cr with checklist: \n",
      "                                                 text  class\n",
      "0  When I saw a high gain (low price) antenna was...      1\n",
      "1  When I saw a high gain (low price) antenna was...      1\n",
      "=====================================\n",
      "augmenting pc with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for pc with checklist: \n",
      "                            text  class\n",
      "0  Limited ring tone selection.      0\n",
      "1  Limited ring tone selection.      0\n",
      "=====================================\n",
      "augmenting sst2 with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for sst2 with checklist: \n",
      "                                              text  class\n",
      "0  steven spielberg brings us another masterpiece      1\n",
      "1  Kortnie Gordillo brings us another masterpiece      1\n",
      "=====================================\n",
      "augmenting subj with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for subj with checklist: \n",
      "                                                 text  class\n",
      "0  yet this highly traditional world is facing pr...      1\n",
      "1  yet this highly traditional world is facing pr...      1\n",
      "=====================================\n",
      "augmenting trec with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for trec with checklist: \n",
      "                                                text  class\n",
      "0  What country has the most time zones , with 11 ?      4\n",
      "1   What country has the most time zones , with 7 ?      4\n",
      "=====================================\n",
      "augmenting kaggle_med with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for kaggle_med with checklist: \n",
      "                                                 text  class\n",
      "0   Right iliopsoas hematoma with associated femo...     22\n",
      "1   Right iliopsoas hematoma with associated femo...     22\n",
      "=====================================\n",
      "augmenting yelp with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for yelp with checklist: \n",
      "                                                 text  class\n",
      "0  This isnt your typical Mexican restaurant that...      5\n",
      "1  This isnt your typical Mexican restaurant that...      5\n",
      "=====================================\n",
      "augmenting bbc with checklist \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for bbc with checklist: \n",
      "                                                 text  class\n",
      "0  I was really looking forward to eating here be...      2\n",
      "1  I was really looking forward to eating here be...      2\n",
      "=====================================\n",
      "augmenting agnews with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for agnews with clare: \n",
      "                                                 text  class\n",
      "0  AP - Albert Demtchenko of Russia won the men's...      2\n",
      "1  . - albert demtchenko of russia won the men ' ...      2\n",
      "=====================================\n",
      "augmenting cardio with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for cardio with clare: \n",
      "                                                 text  class\n",
      "0   One of the most widely studied model systems ...      4\n",
      "1  monday of the most widely mature model system ...      4\n",
      "=====================================\n",
      "augmenting cr with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for cr with clare: \n",
      "                                                 text  class\n",
      "0   Great Design-- Very sleek and touch wheel has...      1\n",
      "1  ##notes design - - very sleek and new wheel ha...      1\n",
      "=====================================\n",
      "augmenting pc with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for pc with clare: \n",
      "                                                 text  class\n",
      "0          fun, cute, easy to set up, light, durable      1\n",
      "1  down world cute , easy to set up , light or ci...      1\n",
      "=====================================\n",
      "augmenting sst2 with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for sst2 with clare: \n",
      "                                                 text  class\n",
      "0  memorable for a peculiar malaise that renders ...      0\n",
      "1  \" for a peculiar malaise that renders its land...      0\n",
      "=====================================\n",
      "augmenting subj with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for subj with clare: \n",
      "                                                 text  class\n",
      "0  an occasionally funny , but overall limp , fis...      0\n",
      "1  an occasionally funny , but overall limp , fis...      0\n",
      "=====================================\n",
      "augmenting trec with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for trec with clare: \n",
      "                                                 text  class\n",
      "0    What should you yell to hail a taxi in Madrid ?      0\n",
      "1  integrated why you yell to someone a taxi in m...      0\n",
      "=====================================\n",
      "augmenting kaggle_med with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for kaggle_med with clare: \n",
      "                                                 text  class\n",
      "0   A 1-month-26-day-old with failure-to-thrive. ...      5\n",
      "1  . 1 - month - marriage - day - old with failur...      5\n",
      "=====================================\n",
      "augmenting yelp with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for yelp with clare: \n",
      "                                                 text  class\n",
      "0  This is the location I frequent most (it helps...      5\n",
      "1  this is the location i frequent most ( it help...      5\n",
      "=====================================\n",
      "augmenting bbc with clare \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "results for bbc with clare: \n",
      "                                                 text  class\n",
      "0  I really love this place.  The girls that work...      4\n",
      "1  tall i love this place . the girls who work he...      4\n",
      "=====================================\n",
      "augmenting agnews with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for agnews with eda: \n",
      "                                                 text  class\n",
      "0  For 35 years, Advanced Micro Devices Inc. (AMD...      4\n",
      "1  For xxxv twelvemonth, encourage micro device I...      4\n",
      "=====================================\n",
      "augmenting cardio with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cardio with eda: \n",
      "                                                 text  class\n",
      "0   This investigation evaluated the potential to...     21\n",
      "1   This investigation appraise the possible pern...     21\n",
      "=====================================\n",
      "augmenting cr with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cr with eda: \n",
      "                                                 text  class\n",
      "0  It's rechargable and supposedly easy to replac...      1\n",
      "1  It's I and course easy it replace though recha...      1\n",
      "=====================================\n",
      "augmenting pc with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for pc with eda: \n",
      "                                                 text  class\n",
      "0  10x stabilized zoom lens, electronic viewfinde...      1\n",
      "1  10x stabilize surge lense, electronic viewfind...      1\n",
      "=====================================\n",
      "augmenting sst2 with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for sst2 with eda: \n",
      "                                                 text  class\n",
      "0  you will emerge with a clearer view of how the...      1\n",
      "1  you will egress with a readable panorama of ho...      1\n",
      "=====================================\n",
      "augmenting subj with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for subj with eda: \n",
      "                                                 text  class\n",
      "0  you needn't be steeped in '50s sociology , pop...      0\n",
      "1  you needn't be to in 'the sociology , films ha...      0\n",
      "=====================================\n",
      "augmenting trec with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for trec with eda: \n",
      "                                                 text  class\n",
      "0  What body of water does the Yukon River empty ...      4\n",
      "1  What body of water soundbox does the Yukon Riv...      4\n",
      "=====================================\n",
      "augmenting kaggle_med with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for kaggle_med with eda: \n",
      "                                     text  class\n",
      "0   Itchy red rash on feet - Tinea Pedis      5\n",
      "1                 Itchy red feet - Pedis      5\n",
      "=====================================\n",
      "augmenting yelp with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for yelp with eda: \n",
      "                                                 text  class\n",
      "0  We drive a little out of the way to go this ta...      4\n",
      "1  We drive little out go this clean and surround...      4\n",
      "=====================================\n",
      "augmenting bbc with eda \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for bbc with eda: \n",
      "                                                 text  class\n",
      "0  I was very sad to see this restaurant go. I wi...      5\n",
      "1  I was sad see this. Anthony best and opens som...      5\n",
      "=====================================\n",
      "augmenting agnews with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for agnews with wordnet: \n",
      "                                                 text  class\n",
      "0  Online retailer Amazon and Microsoft have team...      3\n",
      "1  online retailer amazon and Microsoft have team...      3\n",
      "=====================================\n",
      "augmenting cardio with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cardio with wordnet: \n",
      "                                                 text  class\n",
      "0   Cerebellar hemangioblastoma is a rare cause o...      4\n",
      "1   cerebellar hemangioblastoma is a uncommon get...      4\n",
      "=====================================\n",
      "augmenting cr with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cr with wordnet: \n",
      "                                                 text  class\n",
      "0  just received this camera two days ago and alr...      1\n",
      "1  just obtain this camera ii day agone and alrea...      1\n",
      "=====================================\n",
      "augmenting pc with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for pc with wordnet: \n",
      "                                                 text  class\n",
      "0  Very easy to use.  Takes clear and vivid pictures      1\n",
      "1  Very well-heeled to use.  subscribe net and vi...      1\n",
      "=====================================\n",
      "augmenting sst2 with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for sst2 with wordnet: \n",
      "                                                 text  class\n",
      "0  the only thing scary about feardotcom is that ...      0\n",
      "1  the only matter chilling about feardotcom is t...      0\n",
      "=====================================\n",
      "augmenting subj with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for subj with wordnet: \n",
      "                                                 text  class\n",
      "0  who is murdered &#38 ; who is the murderer . . .       1\n",
      "1  who is remove &#thirty-eight ; who is the mans...      1\n",
      "=====================================\n",
      "augmenting trec with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for trec with wordnet: \n",
      "                                       text  class\n",
      "0  What sea surrounds the Cayman Islands ?      4\n",
      "1     What sea environ the cayman island ?      4\n",
      "=====================================\n",
      "augmenting kaggle_med with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for kaggle_med with wordnet: \n",
      "                                                 text  class\n",
      "0   Skin biopsy, scalp mole.  Darkened mole statu...     38\n",
      "1   scrape biopsy, scalp bulwark.  darkened mol c...     38\n",
      "=====================================\n",
      "augmenting yelp with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for yelp with wordnet: \n",
      "                                                 text  class\n",
      "0  This is the place the Scottsdale pooches come ...      4\n",
      "1  This is the localise the Scottsdale bow-wow ar...      4\n",
      "=====================================\n",
      "augmenting bbc with wordnet \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for bbc with wordnet: \n",
      "                                                 text  class\n",
      "0  This is a new neighborhood breakfast joint in ...      4\n",
      "1  This is a raw neighbourhood breakfast reefer i...      4\n",
      "=====================================\n",
      "augmenting agnews with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for agnews with charswap: \n",
      "                                                 text  class\n",
      "0  With only three games left in the season, the ...      2\n",
      "1  WGth only thee egames left in the seasn, the R...      2\n",
      "=====================================\n",
      "augmenting cardio with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cardio with charswap: \n",
      "                                                 text  class\n",
      "0   To gain insight into corporate activities reg...     21\n",
      "1   Tvo gani vinsight into corpmorate Sactivities...     21\n",
      "=====================================\n",
      "augmenting cr with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cr with charswap: \n",
      "                                                 text  class\n",
      "0  This is truly a Clydesdale among the industry ...      1\n",
      "1  Thi is tuly a Clyedsdale among the industry 3H...      1\n",
      "=====================================\n",
      "augmenting pc with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for pc with charswap: \n",
      "                                                 text  class\n",
      "0  Versatile face plate exchanges, 39 ring tones,...      1\n",
      "1  Verstaile face lpate exchanges, 39 irng tones,...      1\n",
      "=====================================\n",
      "augmenting sst2 with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for sst2 with charswap: \n",
      "                                                 text  class\n",
      "0  it 's a movie that accomplishes so much that o...      1\n",
      "1  it 's a moive that eccomplishes so muhc that B...      1\n",
      "=====================================\n",
      "augmenting subj with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for subj with charswap: \n",
      "                                                 text  class\n",
      "0  the sister kills herself when she finds out th...      1\n",
      "1  the Msister kiMls herself when she fidns out t...      1\n",
      "=====================================\n",
      "augmenting trec with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for trec with charswap: \n",
      "                     text  class\n",
      "0  What are Maid-Rites ?      0\n",
      "1   Wha are Maid-Rites ?      0\n",
      "=====================================\n",
      "augmenting kaggle_med with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for kaggle_med with charswap: \n",
      "                                                 text  class\n",
      "0   Diagnostic laparoscopy and laparoscopic appen...     38\n",
      "1   Diagnostiac laparoscopy and laparoscopic apen...     38\n",
      "=====================================\n",
      "augmenting yelp with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for yelp with charswap: \n",
      "                                                 text  class\n",
      "0  As far as indie record stores, this is truly a...      5\n",
      "1  Bs Sfar as idie rceord stoers, this is zruly a...      5\n",
      "=====================================\n",
      "augmenting bbc with charswap \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for bbc with charswap: \n",
      "                                                 text  class\n",
      "0  This place use to be a Wendy's for years since...      4\n",
      "1  Twis place usHe to be a WendT's for ears sicne...      4\n",
      "=====================================\n",
      "augmenting agnews with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for agnews with deletion: \n",
      "                                                 text  class\n",
      "0  ANDOVER Coach: Ken Maglio (ninth year, 37-46-2...      2\n",
      "1  ANDOVER Coach: Maglio ( year,). Last:. starter...      2\n",
      "=====================================\n",
      "augmenting cardio with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cardio with deletion: \n",
      "                                            text  class\n",
      "0   PVH was classified according to its extent.     16\n",
      "1                    PVH was classified extent.     16\n",
      "=====================================\n",
      "augmenting cr with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cr with deletion: \n",
      "                                                 text  class\n",
      "0  As for the Zen Micro:\\  Cons:  I read that eve...      0\n",
      "1  As for the Zen:\\ :  button is, thats annoying ...      0\n",
      "=====================================\n",
      "augmenting pc with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for pc with deletion: \n",
      "                                      text  class\n",
      "0  Small, light weight, lots of features.      1\n",
      "1                   Small, light weight,.      1\n",
      "=====================================\n",
      "augmenting sst2 with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for sst2 with deletion: \n",
      "                                                 text  class\n",
      "0  spreads itself too thin , leaving these actors...      0\n",
      "1  too thin , , as well as the the , short profou...      0\n",
      "=====================================\n",
      "augmenting subj with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for subj with deletion: \n",
      "                                                 text  class\n",
      "0  he's a rockabilly star who's a legend both bec...      1\n",
      "1  rockabilly star a his great hits and because u...      1\n",
      "=====================================\n",
      "augmenting trec with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for trec with deletion: \n",
      "                                                 text  class\n",
      "0  How many rows of sprocket holes does a roll of...      5\n",
      "1                   How many of holes of film have ?      5\n",
      "=====================================\n",
      "augmenting kaggle_med with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for kaggle_med with deletion: \n",
      "                                                 text  class\n",
      "0   Closed reduction of mandible fractures with E...      7\n",
      "1   Closed reduction fractures Erich bars fixatio...      7\n",
      "=====================================\n",
      "augmenting yelp with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for yelp with deletion: \n",
      "                                                 text  class\n",
      "0  I LOVE White Lion Tea. The company uses great ...      5\n",
      "1  I White. quality of!!! are Tuscany, Peach, Gre...      5\n",
      "=====================================\n",
      "augmenting bbc with deletion \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for bbc with deletion: \n",
      "                                                 text  class\n",
      "0  My boyfrind and I attended here this past week...      5\n",
      "1  boyfrind attended, to our one. went out their ...      5\n",
      "=====================================\n",
      "augmenting agnews with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for agnews with embedding: \n",
      "                                                 text  class\n",
      "0  MILWAUKEE (SportsTicker) - Barry Bonds tries t...      2\n",
      "1  MILWAUKEE (SportsTicker) - Bari Bond endeavour...      2\n",
      "=====================================\n",
      "augmenting cardio with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cardio with embedding: \n",
      "                                                 text  class\n",
      "0   Thus, recognizing the dermatologic manifestat...     23\n",
      "1   Thereby, recognised the dermatologic protesti...     23\n",
      "=====================================\n",
      "augmenting cr with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for cr with embedding: \n",
      "                                         text  class\n",
      "0    really happy with this little camera .       1\n",
      "1  truly jubilant with this petite camera .       1\n",
      "=====================================\n",
      "augmenting pc with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for pc with embedding: \n",
      "                                                 text  class\n",
      "0  Great for  and pics of intricate items as well...      1\n",
      "1  Formidable for  and pics of complicated item a...      1\n",
      "=====================================\n",
      "augmenting sst2 with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for sst2 with embedding: \n",
      "                                          text  class\n",
      "0     the formula is familiar but enjoyable .      1\n",
      "1  the formulas is familiarize but pleasing .      1\n",
      "=====================================\n",
      "augmenting subj with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for subj with embedding: \n",
      "                                                 text  class\n",
      "0   . . . has its moments , but ultimately , its ...      0\n",
      "1   . . . has its times , but finally , its curmu...      0\n",
      "=====================================\n",
      "augmenting trec with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for trec with embedding: \n",
      "                                                 text  class\n",
      "0  What was football star Elroy Hirsch 's nickname ?      3\n",
      "1  Quel was football superstar Elroy Hirsh 's pse...      3\n",
      "=====================================\n",
      "augmenting kaggle_med with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for kaggle_med with embedding: \n",
      "                                          text  class\n",
      "0      Colonoscopy to screen for colon cancer     14\n",
      "1   Colonoscopy to screen for surfboard tumor     14\n",
      "=====================================\n",
      "augmenting yelp with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for yelp with embedding: \n",
      "                                                 text  class\n",
      "0  I was also roped in with a flier on my door, a...      5\n",
      "1  I was similarly roped in with a prospectus on ...      5\n",
      "=====================================\n",
      "augmenting bbc with embedding \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/peyman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/peyman/anaconda3/envs/test2/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J=====================================\n",
      "results for bbc with embedding: \n",
      "                                                 text  class\n",
      "0  There is no shortage of Mexican restaurants in...      5\n",
      "1  There is no imperfection of Wetback restaurant...      5\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "for i in dict_methods:\n",
    "    for j in dict_datasets:        \n",
    "        df = load_data(dict_datasets[j])\n",
    "        df = df.sample(n=1)        \n",
    "        method = dict_methods[i]\n",
    "        print(f'augmenting {j} with {i} \\n ')\n",
    "        res = augment_text(df, method, fraction=1, pct_words_to_swap=0.5, transformations_per_example=1, label_column='class', target_column='text', include_original=True)\n",
    "        print('=====================================')\n",
    "        print(f'results for {j} with {i}: \\n {res}')\n",
    "        print('=====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dict_methods:\n",
    "    for j in dict_datasets:\n",
    "        df = load_data(dict_datasets[j])\n",
    "        if df is None:\n",
    "            continue\n",
    "        df = df.sample(n=1)\n",
    "        method = dict_methods[i]\n",
    "        print(f'augmenting {j} with {i} \\n ')\n",
    "        try:\n",
    "            res = augment_text(df, method, fraction=1, pct_words_to_swap=0.5, transformations_per_example=1, label_column='class', target_column='text', include_original=True)\n",
    "            print('=====================================')\n",
    "            print(f'results for {j} with {i}: \\n {res}')\n",
    "            print('=====================================')\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError in dataset {j} with method {i}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('./data/original/cardio/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/original/cardio/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/cardio/train.csv', header=None, names=['class','text'], dtype={'class': str},skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the data fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.csv',\n",
    "    'cardio': './data/original/cardio/train.csv',\n",
    "    'cr': './data/original/cr/train.csv',\n",
    "    'pc': './data/original/pc/train.csv',    \n",
    "    'sst2': './data/original/sst2/train.csv',\n",
    "    'subj': './data/original/subj/train.csv',\n",
    "    'trec': './data/original/trec/train.csv',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.csv',\n",
    "    'yelp': './data/original/yelp/train.csv',\n",
    "    'bbc': './data/original/bbc/train.csv'\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 20\n",
    "for i in dict_datasets:\n",
    "    df = load_data(dict_datasets[i])\n",
    "    df = df.sample(frac=0.1, random_state=random_state)\n",
    "    df.to_csv(f'./data/original/{i}/train_10.csv', index=False)\n",
    "    df = df.sample(frac=0.2, random_state=random_state)\n",
    "    df.to_csv(f'./data/original/{i}/train_20.csv', index=False)\n",
    "    df = df.sample(frac=0.5, random_state=random_state)\n",
    "    df.to_csv(f'./data/original/{i}/train_50.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort the datasets by the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict_datasets = {}\n",
    "\n",
    "for i in dict_datasets:\n",
    "    len_dataset = len(load_data(dict_datasets[i]))\n",
    "    new_dict_datasets[i] = len_dataset\n",
    "\n",
    "new_dict_datasets = sorted(new_dict_datasets.items(), key=lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cr', 4067),\n",
       " ('kaggle_med', 4245),\n",
       " ('trec', 5452),\n",
       " ('sst2', 7791),\n",
       " ('subj', 9000),\n",
       " ('yelp', 9000),\n",
       " ('bbc', 9000),\n",
       " ('pc', 39879),\n",
       " ('agnews', 120000),\n",
       " ('cardio', 435002)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt',\n",
    "    'bbc': './data/original/bbc/train.txt'\n",
    "}    \n",
    "for dataset_name in dict_datasets:\n",
    "    path = dict_datasets[dataset_name]\n",
    "    data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "    print(f'Loaded {len(data)} rows of data from {dataset_name}')\n",
    "\n",
    "    # Take only one sample from each dataset\n",
    "    data = data.sample(n=1)\n",
    "    print(data)\n",
    "    print('=====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/original/pc/train.txt', delimiter='\\t', header=None, names=['class', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba22cf2efef377717f89a84cea157a0c07bbb13bc3a02b99b621aabdf4befcc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
