{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/agnews/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 titles and descriptions in the dataset full text\n",
    "for i in range(3):\n",
    "    title = df['Title'][i]\n",
    "    description = df['Description'][i]\n",
    "    label = df['Class Index'][i]\n",
    "    num_tokens = description.count(' ') + 1\n",
    "    print('Title: ', title)\n",
    "    print('Description: ', description)\n",
    "    print('Number of tokens: ', num_tokens)\n",
    "    print('Label: ', label)\n",
    "\n",
    "    print('===========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(df)):    \n",
    "    description = df['Description'][i]    \n",
    "    num_tokens = description.count(' ') + 1\n",
    "    tokens.append(num_tokens)\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[['Class Index', 'Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/agnews/train.csv')\n",
    "test_df = pd.read_csv('./data/agnews/test.csv')\n",
    "\n",
    "train_df = train_df[['Class Index', 'Description']]\n",
    "test_df = test_df[['Class Index', 'Description']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savetxt('./data/agnews/train.txt', train_df.values, fmt='%s', delimiter='\\t')\n",
    "np.savetxt('./data/agnews/test.txt', test_df.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med = pd.read_csv('./data/original/kaggle_med/train.txt',delimiter='\\t', header=None, names=['class', 'text'])\n",
    "kaggle_med['text'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[1]) \n",
    "kaggle_med['class'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == '7.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == '6.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.text.loc[kaggle_med['class'] == 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train = kaggle_med.drop(kaggle_med[(kaggle_med['class'] == 'and') | (kaggle_med['class'] == '6.') | (kaggle_med['class'] == '7.')].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/original/kaggle_med/train.txt', new_df_train.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med = pd.read_csv('./data/original/kaggle_med/test.txt',delimiter='\\t', header=None, names=['class', 'text'])\n",
    "kaggle_med['text'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[1]) \n",
    "kaggle_med['class'] = kaggle_med['class'].apply(lambda x: x.split(' ',1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_med.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/kaggle_med/train.txt',delimiter='\\t', header=None, names=['class', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated()]\n",
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df['text'].duplicated()]\n",
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(duplicates)):\n",
    "    print(duplicates['text'].iloc[i])\n",
    "    print('===========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.groupby('text').apply(lambda x: pd.Series({\n",
    "    'class': list(x['class']),\n",
    "    'count': len(x)\n",
    "})).reset_index()\n",
    "duplicates = duplicates[duplicates.duplicated(subset='class')]\n",
    "for i in range(len(duplicates)):\n",
    "    count = duplicates['count'].iloc[i]\n",
    "    if count > 2:\n",
    "        print('text: ',duplicates['text'].iloc[i])\n",
    "        print('class: ',duplicates['class'].iloc[i])\n",
    "        print('count: ',duplicates['count'].iloc[i])\n",
    "        print('===========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/cardio/train.txt',delimiter='\\t', header=None, names=['class', 'text'])\n",
    "duplicates = df[df.duplicated()]\n",
    "len(duplicates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new session\n",
    "### augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',    \n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import snoop\n",
    "# @snoop(depth=2)\n",
    "def aug_samples(dataset_name, method_names, fraction, pct_words_to_swap, transformations_per_example, include_original):\n",
    "    data = load_data(dict_datasets[dataset_name])\n",
    "    print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "    #data = data.sample(frac=fraction)\n",
    "    #print(f'Sampled {len(data)} rows with fraction={fraction}')\n",
    "    for method in method_names:\n",
    "        if method not in dict_methods:\n",
    "            raise ValueError(f\"Method '{method}' is not supported\")\n",
    "        augmented_data = augment_text(\n",
    "            data,\n",
    "            dict_methods[method],\n",
    "            fraction=fraction,\n",
    "            pct_words_to_swap=pct_words_to_swap,\n",
    "            transformations_per_example=transformations_per_example,\n",
    "            label_column='class',\n",
    "            target_column='text',\n",
    "            include_original=include_original\n",
    "        )\n",
    "        augmented_data = augmented_data[['class', 'text']]\n",
    "        np.savetxt(\n",
    "            f'./data/augmented/{dataset_name}/{fraction}_{method}_{transformations_per_example}.txt',\n",
    "            augmented_data.values,\n",
    "            fmt='%s',\n",
    "            delimiter='\\t'\n",
    "        )\n",
    "        print(f'number of augmented samples: {len(augmented_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('agnews', ['eda'], 0.001, 0.5, 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_samples(dataset_name, method_names, fraction, pct_words_to_swap, transformations_per_example, include_original):\n",
    "    data = load_data(dict_datasets[dataset_name])\n",
    "    methods = method_names\n",
    "    for method in methods:\n",
    "        augmented_data = augment_text( data, method, fraction=fraction, pct_words_to_swap=pct_words_to_swap, transformations_per_example=transformations_per_example,\n",
    "                                                            label_column='class', target_column='text', include_original=include_original)\n",
    "        augmented_data = augmented_data[['class','text']]\n",
    "        np.savetxt(f'./data/augmented/{dataset_name}/{fraction}_{method}_{transformations_per_example}.txt', augmented_data.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('pc', ['checklist_augmenter'], 0.001, 0.5, 2, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dict_datasets['pc'])\n",
    "df = data.copy( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[~(df['text'].isin(['','none','nan','null','NaN','NULL','Null','None','NONE']))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/original/pc/train_clean.txt', df.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('pc_clean', ['checklist'], 0.001, 0.5, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/yelp/yelp.csv')\n",
    "df = df[['stars', 'text']]\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, \"text\"] = re.sub(r\"[\\\\']\", \"\", df.loc[i, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):    \n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\t+\", \" \", text)\n",
    "    text = re.sub(r\"\\r+\", \" \", text)\n",
    "    text = re.sub(r\"/\\sThe+\", \"The\", text)\n",
    "    text = re.sub(r\"[\\\\']+\", \"'\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train.to_csv('./data/original/yelp/train.csv', index=False)\n",
    "test.to_csv('./data/original/yelp/test.csv', index=False)\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x: clean_text(x))\n",
    "train['text'] = train['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "\n",
    "np.savetxt('./data/original/yelp/train.txt', train.values, fmt='%s', delimiter='\\t')\n",
    "np.savetxt('./data/original/yelp/test.txt', test.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./data/original/yelp/train.txt',delimiter='\\t', header=None, names=['class', 'text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new session\n",
    "### augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'embedding': 'embedding_augmenter',\n",
    "    'deletion': 'deletion_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_samples(dataset_name, method_names, fraction, pct_words_to_swap, transformations_per_example, include_original):\n",
    "    path = dict_datasets[dataset_name]\n",
    "    data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "    data = data.sample(frac=fraction)\n",
    "    print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')\n",
    "    #data = data.sample(frac=fraction)\n",
    "    #print(f'Sampled {len(data)} rows with fraction={fraction}')\n",
    "    for method in method_names:\n",
    "        if method not in dict_methods:\n",
    "            raise ValueError(f\"Method '{method}' is not supported\")\n",
    "        augmented_data = augment_text(\n",
    "            data,\n",
    "            dict_methods[method],\n",
    "            fraction=1,\n",
    "            pct_words_to_swap=pct_words_to_swap,\n",
    "            transformations_per_example=transformations_per_example,\n",
    "            label_column='class',\n",
    "            target_column='text',\n",
    "            include_original=include_original\n",
    "        )\n",
    "        augmented_data = augmented_data[['class', 'text']]\n",
    "        np.savetxt(\n",
    "            f'./data/augmented/{dataset_name}/{fraction}_{method}_{transformations_per_example}.txt',\n",
    "            augmented_data.values,\n",
    "            fmt='%s',\n",
    "            delimiter='\\t'\n",
    "        )\n",
    "        print(f'number of augmented samples: {len(augmented_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('yelp', ['eda'], 0.001, 0.5, 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/original/yelp/train.txt',delimiter='\\t', header=None, names=['class', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4284,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "from functions1 import *\n",
    "method_names = ['eda']\n",
    "dataset_name = 'yelp'\n",
    "fraction = 0.001\n",
    "path = dict_datasets[dataset_name]\n",
    "data = pd.read_csv(path, delimiter='\\t', header=None, names=['class', 'text'])\n",
    "data = data.sample(frac=fraction)\n",
    "print(f'Loaded {len(data)} rows of data from {dict_datasets[dataset_name]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'embedding': 'embedding_augmenter',\n",
    "    'deletion': 'deletion_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "from functions import *\n",
    "def aug_samples(name,frac):\n",
    "    path = dict_datasets[name]\n",
    "    data = load_data(path)\n",
    "    data = data.sample(frac=frac)\n",
    "    print(f'Loaded {len(data)} rows of data from {name}')\n",
    "    #methods = ['eda_augmenter','wordnet_augmenter','aeda_augmenter','backtranslation_augmenter']\n",
    "    methods = ['checklist_augmenter']\n",
    "    for method in methods:\n",
    "        augmented_data = augment_text(data, method,fraction=1,pct_words_to_swap=0.2 ,transformations_per_example=1,\n",
    "                    label_column='class',target_column='text',include_original=True)\n",
    "        augmented_data = augmented_data[['class','text']]\n",
    "        np.savetxt(f'./data/augmented/{name}/{method}_aug.txt', augmented_data.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('pc',0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textattack\n",
    "import logging\n",
    "from functions import *\n",
    "\n",
    "# Define the input paths for the datasets\n",
    "dict_datasets = {\n",
    "    'agnews': './data/original/agnews/train.txt',\n",
    "    'cardio': './data/original/cardio/train.txt',\n",
    "    'cr': './data/original/cr/train.txt',\n",
    "    'pc': './data/original/pc/train.txt',\n",
    "    'pc_clean': './data/original/pc/train_clean.txt',\n",
    "    'sst2': './data/original/sst2/train.txt',\n",
    "    'subj': './data/original/subj/train.txt',\n",
    "    'trec': './data/original/trec/train.txt',\n",
    "    'kaggle_med': './data/original/kaggle_med/train.txt',\n",
    "    'yelp': './data/original/yelp/train.txt'\n",
    "}\n",
    "\n",
    "# Define the augmentation methods and their names\n",
    "dict_methods = {\n",
    "    'aeda': 'aeda_augmenter',\n",
    "    'backtranslation': 'backtranslation_augmenter',\n",
    "    'checklist': 'checklist_augmenter',\n",
    "    'clare': 'clare_augmenter',\n",
    "    'eda': 'eda_augmenter',\n",
    "    'wordnet': 'wordnet_augmenter'\n",
    "}\n",
    "\n",
    "aug_num = [1,2,4]\n",
    "aug_frac = [0.1,0.2,0.5,1]\n",
    "\n",
    "logging.basicConfig(filename='augment.log', filemode='a', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "def aug_samples(name, frac):\n",
    "    path = dict_datasets[name]\n",
    "    data = load_data(path)\n",
    "    try:\n",
    "        data = data.sample(frac=frac)\n",
    "    except:\n",
    "        data = data.sample(n=1)\n",
    "    logging.info(f'Loaded {len(data)} rows of data from {name}')\n",
    "    \n",
    "    #methods = ['eda_augmenter','wordnet_augmenter','aeda_augmenter','backtranslation_augmenter']\n",
    "    methods = ['checklist_augmenter', 'eda_augmenter', 'wordnet_augmenter', 'aeda_augmenter', 'backtranslation_augmenter', 'clare_augmenter']\n",
    "    \n",
    "    for method in methods:\n",
    "        augmented_data = augment_text(data, method, fraction=1, pct_words_to_swap=0.2, transformations_per_example=1,\n",
    "                                      label_column='class', target_column='text', include_original=True)\n",
    "        augmented_data = augmented_data[['class','text']]\n",
    "        np.savetxt(f'./data/augmented/{name}/{method}_aug.txt', augmented_data.values, fmt='%s', delimiter='\\t')\n",
    "        logging.info(f'Augmented {len(augmented_data)} rows of data from {name} using {method} method')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_samples('yelp', 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = dict_datasets['yelp']\n",
    "path = './data/original/yelp/train.csv'\n",
    "df = load_data(path)\n",
    "#df = pd.read_csv(path,  names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['text'].apply(lambda x: x.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(df['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads data from a txt file.\n",
    "    \"\"\"\n",
    "    # check file format\n",
    "    if path.endswith('.txt'):\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep='|', header=None, names=['text'])\n",
    "        except:\n",
    "            df = pd.read_csv(path, sep='\\t', header=None, names=['text'])\n",
    "        df['class'], df['text'] = zip(*df['text'].apply(lambda x: x.split('\\t', 1)))\n",
    "    else:\n",
    "        df = pd.read_csv(path, header=None, names=['class', 'text'])\n",
    "\n",
    "    return df[['class', 'text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/original/yelp/train.csv'\n",
    "df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads data from a txt file.\n",
    "    \"\"\"\n",
    "    # check file format\n",
    "    if path.endswith('.txt'):\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep='|', header=None, names=['text'])\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep='\\t', header=None, names=['text'])\n",
    "            except:\n",
    "                df = pd.read_csv(path, header=None, names=['text'])\n",
    "                df['class'] = df['text'].apply(lambda x: x.split(' ',1)[0])\n",
    "                df['text'] = df['text'].apply(lambda x: x.split(' ',1)[1])\n",
    "        else:\n",
    "            df['class'], df['text'] = zip(*df['text'].apply(lambda x: x.split('\\t', 1)))\n",
    "    else:\n",
    "        df = pd.read_csv(path, header=None, names=['class', 'text'],skiprows=1)\n",
    "\n",
    "    return df[['class', 'text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/original/bbc/train.csv'\n",
    "df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbc = pd.read_csv('./data/original/bbc/bbc.csv', header=None, names=['class', 'text'],skiprows=1)\n",
    "# bbc['class'] = bbc['class'].map({'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4})\n",
    "# train, test = train_test_split(bbc, test_size=0.1, random_state=42)\n",
    "\n",
    "# train.to_csv('./data/original/bbc/train.csv', index=False)\n",
    "# test.to_csv('./data/original/bbc/test.csv', index=False)\n",
    "\n",
    "# np.savetxt('./data/original/bbc/train.txt', train.values, fmt='%s', delimiter='\\t')\n",
    "# np.savetxt('./data/original/bbc/test.txt', test.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/original/bbc/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>brown and blair face new rift claims for the u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>small firms  hit by rising costs  rising fuel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>spirit awards hail sideways the comedy sideway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>microsoft releases patches microsoft has warne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>arsenal through on penalties arsenal win 4-2 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>3</td>\n",
       "      <td>pearce keen on succeeding keegan joint assista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2</td>\n",
       "      <td>blair blasts tory spending plans tony blair ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3</td>\n",
       "      <td>moya fights back for indian title carlos moya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2</td>\n",
       "      <td>crucial decision on super-casinos a decision o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2</td>\n",
       "      <td>blair says mayor should apologise tony blair h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                               text\n",
       "0        2  brown and blair face new rift claims for the u...\n",
       "1        0  small firms  hit by rising costs  rising fuel ...\n",
       "2        1  spirit awards hail sideways the comedy sideway...\n",
       "3        4  microsoft releases patches microsoft has warne...\n",
       "4        3  arsenal through on penalties arsenal win 4-2 o...\n",
       "..     ...                                                ...\n",
       "218      3  pearce keen on succeeding keegan joint assista...\n",
       "219      2  blair blasts tory spending plans tony blair ha...\n",
       "220      3  moya fights back for indian title carlos moya ...\n",
       "221      2  crucial decision on super-casinos a decision o...\n",
       "222      2  blair says mayor should apologise tony blair h...\n",
       "\n",
       "[223 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class'] = train_df['Category'].map({'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4})\n",
    "test_df['class'] = test_df['Category'].map({'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4})\n",
    "\n",
    "# train_df = train_df[['class', 'Text']]\n",
    "# test_df = test_df[['class', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):    \n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\t+\", \" \", text)\n",
    "    text = re.sub(r\"\\r+\", \" \", text)\n",
    "    text = re.sub(r\"/\\sThe+\", \"The\", text)\n",
    "    #text = re.sub(r\"(\\\\')+\", \"'\", text)\n",
    "    text = re.sub(r'\\\\+', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_csv('./data/original/yelp/yelp.csv')\n",
    "yelp = yelp[['stars', 'text']]\n",
    "yelp['text'] = yelp['text'].map(clean_text)\n",
    "yelp.columns = ['class', 'text']\n",
    "yelp['text'] = yelp['text'].apply(lambda x: str(x).replace(\"\\\\'s\",\"'s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>3</td>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>4</td>\n",
       "      <td>Should be called house of deliciousness! I cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>4</td>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2</td>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "0         5  My wife took me here on my birthday for breakf...\n",
       "1         5  I have no idea why some people give bad review...\n",
       "2         4  love the gyro plate. Rice is so good and I als...\n",
       "3         5  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n",
       "4         5  General Manager Scott Petello is a good egg!!!...\n",
       "...     ...                                                ...\n",
       "9995      3  First visit...Had lunch here today - used my G...\n",
       "9996      4  Should be called house of deliciousness! I cou...\n",
       "9997      4  I recently visited Olive and Ivy for business ...\n",
       "9998      2  My nephew just moved to Scottsdale recently so...\n",
       "9999      5  4-5 locations.. all 4.5 star average.. I think...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(yelp, test_size=0.1, random_state=42)\n",
    "train.to_csv('./data/original/bbc/train.csv', index=False)\n",
    "test.to_csv('./data/original/bbc/test.csv', index=False)\n",
    "\n",
    "np.savetxt('./data/original/bbc/train.txt', train.values, fmt='%s', delimiter='\\t')\n",
    "np.savetxt('./data/original/bbc/test.txt', test.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (10):\n",
    "    print(yelp['class'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The upstairs theater is nice enough. There are a few nice semi-upscale restaurants here. It\\'s also got a really good swanky (and somewhat hidden) bar here.  So why only 2 stars? Well, because it smells like this is where all the diarrhea in the world goes to die... especially the parking structure. If you can escape the smell though, this is a nice spot for a date. The problem with that is when you go walk her to her car, you have to try and figure out how to have a good night kiss while your nose is telling your brain that you are swimming in a septic tank. The new Waterfront mall corridor stinks like this as well. Maybe that\\'s where the term \"filthy rich\" comes from. What do these people eat?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].iloc[2248]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'There are a few nice semi-upscale restaurants here. It\\'s also got a really good swanky (and somewhat hidden) bar here.'\n",
    "new_txt  = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba22cf2efef377717f89a84cea157a0c07bbb13bc3a02b99b621aabdf4befcc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
